{"cells":[{"cell_type":"markdown","metadata":{"id":"FVw_sHiQav9Y"},"source":["### **Connecting with Drive**"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2984,"status":"ok","timestamp":1738650447684,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"},"user_tz":-240},"id":"3fuk29ccavxQ","outputId":"b3e4e92f-fccb-4cc1-8852-5ada927d2e18"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["#Reading the training data Subject\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"OWU0BwfKaqMa"},"source":["### **Importing All needed libraries**"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1738650447684,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"},"user_tz":-240},"id":"zvMx9ro8ZntG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fe3834a5-5971-4732-c3e4-15fe23e4ea39"},"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:tensorflow:An interactive session is already active. This can cause out-of-memory errors or some other unexpected errors (due to the unpredictable timing of garbage collection) in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s). Please use `tf.Session()` if you intend to productionize.\n"]}],"source":["#Importing all needed libraries\n","import pandas as pd\n","import numpy as np #Matric math\n","import tensorflow as tf #ML\n","from tensorflow.python.framework import ops\n","from random import randint\n","from numpy import array\n","from numpy import argmax\n","import keras.backend as K\n","from tensorflow.keras import models\n","from numpy import array_equal\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import LSTM, Bidirectional\n","from tensorflow.keras.layers import Dense, Flatten\n","from tensorflow.keras import Input\n","from tensorflow.keras.layers import TimeDistributed\n","from tensorflow.keras.layers import RepeatVector\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.utils import plot_model\n","import matplotlib.pyplot as plt\n","import sys\n","import os\n","from scipy.io import loadmat\n","from scipy.io import loadmat\n","\n","# sys.path.append(os.path.abspath(\"/Users/henda/anaconda3/Lib/site-packages\"))\n","# from rnn_utils import *\n","# from public_tests import *\n","ops.reset_default_graph()\n","\n","tf.compat.v1.reset_default_graph() #Clearning cache\n","sess=tf.compat.v1.InteractiveSession()\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f27nYeB4a4F9"},"source":["### **Define Task**"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1738650447684,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"},"user_tz":-240},"id":"rSSsBybQa36e"},"outputs":[],"source":["# Split data into support and query sets\n","def split_task_data(data, sequence_length=15, n_features=96, split_ratio=0.8):\n","    # X = data[:, 36:58]\n","    # Z = data[:, 58:82]\n","    # Z = data[:, 0:60]  # Columns 0 to 59 â†’ 60 columns\n","    # X = data[:, 60:82] # Columns 60 to 81 â†’ 22 columns\n","    Z = data[:, 0:96]  # Columns 0 to 59 â†’ 60 columns\n","    X = data[:, 96:118] # Columns 60 to 81 â†’ 22 columns\n","    n_sequences = len(Z) - sequence_length + 1\n","\n","    inputs = np.zeros((n_sequences, sequence_length, n_features))\n","    outputs = np.zeros((n_sequences, X.shape[1]))\n","\n","    for j in range(n_sequences):\n","        inputs[j] = Z[j:j + sequence_length]\n","        outputs[j] = X[j + sequence_length - 1]\n","\n","    # Split into support and query sets\n","    split_index = int(len(inputs) * split_ratio)\n","    support_x, query_x = inputs[:split_index], inputs[split_index:]\n","    support_y, query_y = outputs[:split_index], outputs[split_index:]\n","\n","      # Debugging: Print actual sizes after splitting\n","    print(f\"ðŸš€ Debug: Final Support X = {inputs[:split_index].shape}\")\n","    print(f\"ðŸš€ Debug: Final Query X = {inputs[split_index:].shape}\")\n","\n","    return support_x, support_y, query_x, query_y\n","\n","# Prepare tasks for MAML\n","def prepare_tasks(subjects, base_path, sequence_length=15, n_features=96):\n","    tasks = []\n","    for subject in subjects:\n","        data_path = f\"{base_path}/S{subject}_E1.mat\"\n","        subject_data = loadmat(data_path)[\"Data\"]\n","        print(f\"Shape of subject {subject}: {subject_data.shape}\")\n","        support_x, support_y, query_x, query_y = split_task_data(subject_data, sequence_length, n_features)\n","        print(f\"Shape of support_x for subject {subject}: {support_x.shape}\")\n","        print(f\"Shape of support_y for subject {subject}: {support_y.shape}\")\n","        print(f\"Shape of query_x for subject {subject}: {query_x.shape}\")\n","        print(f\"Shape of query_y for subject {subject}: {query_y.shape}\")\n","        tasks.append((support_x, support_y, query_x, query_y))\n","\n","    return tasks\n","\n",""]},{"cell_type":"markdown","metadata":{"id":"3O4YAmlwccNi"},"source":["### **build model**"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1738650447684,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"},"user_tz":-240},"id":"YmPV-5jvccB0"},"outputs":[],"source":["from tensorflow.keras import layers, Model, Input\n","\n","def build_model(input_features=96, timesteps=15, output_dim=22):\n","    inputs = Input(shape=(timesteps, input_features))\n","\n","    # CNN Layers\n","    x = layers.Conv1D(32, kernel_size=3, strides=1, padding='same', activation='relu')(inputs)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.MaxPooling1D(pool_size=2, strides=1, padding='same')(x)\n","\n","    x = layers.Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.MaxPooling1D(pool_size=2, strides=1, padding='same')(x)\n","\n","    x = layers.Conv1D(128, kernel_size=3, strides=1, padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.MaxPooling1D(pool_size=2, strides=1, padding='same')(x)\n","\n","    # LSTM Layers\n","    x = layers.Bidirectional(layers.LSTM(500, return_sequences=True))(x)\n","    x = layers.Dropout(0.2)(x)\n","\n","    x = layers.Bidirectional(layers.LSTM(500))(x)\n","    x = layers.Dropout(0.2)(x)\n","\n","    # Fully Connected Layer\n","    outputs = layers.Dense(output_dim, activation='linear')(x)\n","    return Model(inputs, outputs)\n","\n","\n","# # Build LSTM-based model\n","# def build_model(input_features=24, timesteps=15, output_dim=22):\n","#     inputs = Input(shape=(timesteps, input_features))\n","#     x = layers.Bidirectional(layers.LSTM(64, return_sequences=False))(inputs)\n","#     outputs = layers.Dense(output_dim, activation='linear')(x)\n","#     return Model(inputs, outputs)\n"]},{"cell_type":"markdown","metadata":{"id":"Xko9re4oc81e"},"source":["### **Training Model framework**"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1738650447684,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"},"user_tz":-240},"id":"zk-Pkrfmc8rn"},"outputs":[],"source":["# Inner loop: Task-specific updates\n","def inner_loop(model, support_x, support_y, loss_fn, inner_lr):\n","    \"\"\"\n","    Perform task-specific updates (inner loop) using the support set.\n","    Returns updated weights and loss.\n","    \"\"\"\n","    with tf.GradientTape() as tape:\n","        predictions = model(support_x, training=True)\n","        loss = loss_fn(support_y, predictions)\n","    grads = tape.gradient(loss, model.trainable_variables)\n","    updated_weights = [w - inner_lr * g for w, g in zip(model.trainable_variables, grads)]\n","    return updated_weights, loss\n","\n","\n","# Outer loop: Meta-optimization\n","def maml_step(model, tasks, loss_fn, meta_optimizer, inner_lr):\n","    \"\"\"\n","    Perform a single meta-update step for MAML using the given tasks.\n","    \"\"\"\n","    meta_grads = [tf.zeros_like(var) for var in model.trainable_variables]\n","\n","    for support_x, support_y, query_x, query_y in tasks:\n","        # Perform inner loop to get updated weights\n","        updated_weights, _ = inner_loop(model, support_x, support_y, loss_fn, inner_lr)\n","\n","        # Compute query loss using updated weights\n","        with tf.GradientTape() as tape:\n","            # Temporarily set the model's weights to the updated weights\n","            query_predictions = model(query_x, training=True)\n","            query_loss = loss_fn(query_y, query_predictions)\n","\n","        # Compute gradients with respect to the original weights\n","        task_grads = tape.gradient(query_loss, model.trainable_variables)\n","\n","        # Accumulate gradients across all tasks\n","        meta_grads = [meta_grad + task_grad for meta_grad, task_grad in zip(meta_grads, task_grads)]\n","\n","    # Apply meta-gradient updates\n","    meta_grads = [grad / len(tasks) for grad in meta_grads]  # Average gradients\n","    meta_optimizer.apply_gradients(zip(meta_grads, model.trainable_variables))\n"]},{"cell_type":"markdown","metadata":{"id":"TpUgnKUbfcN3"},"source":["### **Train Model**"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1738650447684,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"},"user_tz":-240},"id":"pt3qXJx9fb-G"},"outputs":[],"source":["# MAML Training Loop\n","def maml_train(model, tasks, epochs, inner_lr, meta_lr):\n","    loss_fn = tf.keras.losses.MeanSquaredError()\n","    meta_optimizer = tf.keras.optimizers.Adam(meta_lr)\n","\n","    for epoch in range(epochs):\n","        maml_step(model, tasks, loss_fn, meta_optimizer, inner_lr)\n","        print(f\"Epoch {epoch + 1}/{epochs} completed\")\n","\n","###############   MAML +Ensemble learning ###########\n","\n","def maml_train_ensemble(num_models, tasks, epochs, inner_lr, meta_lr):\n","    \"\"\"\n","    Train multiple MAML models (ensemble members) on the given tasks.\n","\n","    Parameters:\n","    - num_models: Number of models in the ensemble.\n","    - tasks: List of tasks for meta-training.\n","    - epochs: Number of meta-training epochs.\n","    - inner_lr: Inner-loop learning rate.\n","    - meta_lr: Meta-learning rate for the outer loop.\n","\n","    Returns:\n","    - ensemble: List of trained models.\n","    \"\"\"\n","    ensemble = []\n","\n","    for i in range(num_models):\n","        print(f\"Training model {i + 1}/{num_models}...\")\n","        model = build_model()  # Use the CNN-LSTM model\n","        maml_train(model, tasks, epochs, inner_lr, meta_lr)  # Meta-train the model\n","        ensemble.append(model)\n","\n","    return ensemble\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LHN3FbMa-BJS"},"source":["### **Testing**"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1738650447684,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"},"user_tz":-240},"id":"TEZ9xPgr-A9L"},"outputs":[],"source":["# # Fine-Tune Each Model\n","# def fine_tune_ensemble(ensemble, support_x, support_y, loss_fn, inner_lr, fine_tune_steps=1):\n","#     fine_tuned_weights = []\n","#     for i, model in enumerate(ensemble):\n","#         print(f\"Fine-tuning model {i + 1}/{len(ensemble)}...\")\n","#         updated_weights = model.trainable_variables\n","#         for _ in range(fine_tune_steps):\n","#             updated_weights, _ = inner_loop(model, support_x, support_y, loss_fn, inner_lr)\n","#         fine_tuned_weights.append(updated_weights)\n","#     return fine_tuned_weights\n","\n","def fine_tune_ensemble(ensemble, support_x, support_y, loss_fn, inner_lr, fine_tune_steps=1, batch_size=512):\n","    \"\"\"\n","    Fine-tune the ensemble models using a smaller batch size to avoid GPU memory errors.\n","    \"\"\"\n","    fine_tuned_weights = []\n","\n","    for i, model in enumerate(ensemble):\n","        print(f\"ðŸš€ Fine-tuning model {i + 1}/{len(ensemble)}...\")\n","\n","        updated_weights = model.trainable_variables\n","        dataset = tf.data.Dataset.from_tensor_slices((support_x, support_y)).batch(batch_size)  # âœ… Use batching\n","\n","        for _ in range(fine_tune_steps):\n","            for batch_x, batch_y in dataset:  # âœ… Process in small batches\n","                updated_weights, _ = inner_loop(model, batch_x, batch_y, loss_fn, inner_lr)\n","\n","        fine_tuned_weights.append(updated_weights)\n","\n","    return fine_tuned_weights\n","\n","\n","    # Combine Predictions from Ensemble\n","def ensemble_predict(ensemble, fine_tuned_weights, query_x):\n","    all_predictions = []\n","    for i, model in enumerate(ensemble):\n","        print(f\"Predicting with model {i + 1}/{len(ensemble)}...\")\n","        predictions = model(query_x, training=True)\n","        all_predictions.append(predictions)\n","    ensemble_predictions = np.mean(all_predictions, axis=0)\n","    return ensemble_predictions\n","\n","def ensemble_predict_weighted(ensemble, fine_tuned_weights, query_x, weights):\n","    \"\"\"\n","    Combine predictions from the ensemble using weighted averaging.\n","\n","    Parameters:\n","    - ensemble: List of models.\n","    - fine_tuned_weights: Fine-tuned weights for each model.\n","    - query_x: Input features for the query set.\n","    - weights: List of weights for each model.\n","\n","    Returns:\n","    - ensemble_predictions: Weighted average predictions.\n","    \"\"\"\n","    all_predictions = []\n","    for i, model in enumerate(ensemble):\n","        print(f\"Predicting with model {i + 1}/{len(ensemble)}...\")\n","        predictions = model(query_x, training=True)\n","        all_predictions.append(predictions)\n","\n","    # Convert to numpy array for weighted averaging\n","    all_predictions = np.array(all_predictions)  # Shape: (num_models, batch_size, output_dim)\n","    weights = np.array(weights).reshape(-1, 1, 1)  # Reshape to broadcast\n","\n","    # Weighted average\n","    ensemble_predictions = np.sum(all_predictions * weights, axis=0) / np.sum(weights)\n","    return ensemble_predictions\n","\n","def ensemble_predict_closest(ensemble, fine_tuned_weights, query_x, query_y):\n","    \"\"\"\n","    Combine predictions from the ensemble by selecting the model closest to actual values.\n","\n","    Parameters:\n","    - ensemble: List of models.\n","    - fine_tuned_weights: Fine-tuned weights for each model.\n","    - query_x: Input features for the query set.\n","    - query_y: Ground truth values for the query set.\n","\n","    Returns:\n","    - ensemble_predictions: Predictions closest to the actual values.\n","    \"\"\"\n","    all_predictions = []\n","    errors = []\n","\n","    for i, model in enumerate(ensemble):\n","        print(f\"Predicting with model {i + 1}/{len(ensemble)}...\")\n","        predictions = model(query_x, training=True)\n","        all_predictions.append(predictions)\n","\n","        # Calculate mean squared error for this model\n","        error = np.mean((predictions - query_y) ** 2, axis=0)  # Error per DOF\n","        errors.append(np.sum(error))  # Total error for this model\n","\n","    # Find the model with the least error\n","    best_model_index = np.argmin(errors)\n","    print(f\"Best model is model {best_model_index + 1} with error {errors[best_model_index]:.4f}\")\n","\n","    # Use the best model's predictions\n","    return all_predictions[best_model_index]\n","\n","def ensemble_predict_error_weighted(ensemble, fine_tuned_weights, query_x, query_y):\n","    \"\"\"\n","    Combine predictions from the ensemble using error-weighted averaging.\n","\n","    Parameters:\n","    - ensemble: List of models.\n","    - fine_tuned_weights: Fine-tuned weights for each model.\n","    - query_x: Input features for the query set.\n","    - query_y: Ground truth values for the query set.\n","\n","    Returns:\n","    - ensemble_predictions: Error-weighted average predictions.\n","    \"\"\"\n","    all_predictions = []\n","    errors = []\n","\n","    for i, model in enumerate(ensemble):\n","        print(f\"Predicting with model {i + 1}/{len(ensemble)}...\")\n","        predictions = model(query_x, training=True)\n","        all_predictions.append(predictions)\n","\n","        # Calculate mean squared error for this model\n","        error = np.mean((predictions - query_y) ** 2, axis=0)  # Error per DOF\n","        errors.append(np.sum(error))  # Total error for this model\n","\n","    # Compute weights inversely proportional to errors\n","    weights = 1 / (np.array(errors) + 1e-8)  # Avoid division by zero\n","    weights = weights / np.sum(weights)  # Normalize weights\n","\n","    # Convert predictions to numpy for weighted averaging\n","    all_predictions = np.array(all_predictions)  # Shape: (num_models, batch_size, output_dim)\n","\n","    # Weighted average\n","    ensemble_predictions = np.sum(all_predictions * weights[:, None, None], axis=0)\n","    return ensemble_predictions\n","\n","def ensemble_predict_weighted_performance(ensemble, fine_tuned_weights, query_x, query_y):\n","    all_predictions = []\n","    errors = []\n","\n","    for i, model in enumerate(ensemble):\n","        print(f\"Predicting with model {i + 1}/{len(ensemble)}...\")\n","        predictions = model(query_x, training=True)\n","        if predictions is None:\n","            print(f\"Error: Model {i + 1} returned None predictions.\")\n","            return None\n","        all_predictions.append(predictions)\n","\n","        # Calculate mean squared error for this model\n","        error = np.mean((predictions - query_y) ** 2, axis=0)  # Error per DOF\n","        errors.append(np.sum(error))  # Total error for this model\n","\n","    # Compute weights inversely proportional to errors\n","    weights = 1 / (np.array(errors) + 1e-8)  # Avoid division by zero\n","    weights = weights / np.sum(weights)  # Normalize weights\n","\n","    # Convert predictions to numpy array for weighted averaging\n","    all_predictions = np.array(all_predictions)  # Shape: (num_models, batch_size, output_dim)\n","    if all_predictions.size == 0:\n","        print(\"Error: No predictions available for ensemble. Check models or input data.\")\n","        return None\n","\n","    # Weighted average\n","    ensemble_predictions = np.sum(all_predictions * weights[:, None, None], axis=0)\n","    print(f\"Model Weights (based on performance): {weights}\")\n","    return ensemble_predictions\n","\n","\n","# # Fine-tune on a new subject\n","# def fine_tune(model, support_x, support_y, loss_fn, inner_lr, fine_tune_steps=1):\n","#     \"\"\"\n","#     Fine-tune the model on the support set for a new task.\n","#     Returns the updated weights.\n","#     \"\"\"\n","#     updated_weights = model.trainable_variables\n","#     for _ in range(fine_tune_steps):\n","#         updated_weights, _ = inner_loop(model, support_x, support_y, loss_fn, inner_lr)\n","#     return updated_weights\n","\n","\n","# # Evaluate on a new subject\n","# def evaluate_model(model, updated_weights, query_x, query_y):\n","#     \"\"\"\n","#     Evaluate the model on the query set using the updated weights.\n","#     \"\"\"\n","#     # Temporarily set the model's weights to the updated weights\n","#     query_predictions = model(query_x, training=True)\n","#     correlations = [np.corrcoef(query_y[:, i], query_predictions[:, i])[0, 1] for i in range(query_y.shape[1])]\n","#     avg_correlation = np.mean(correlations)\n","#     print(f\"Average Pearson correlation: {avg_correlation}\")\n","#     return avg_correlation\n","\n"]},{"cell_type":"markdown","metadata":{"id":"q81rczMMDwJR"},"source":["### **Plot the result**"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1738650447685,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"},"user_tz":-240},"id":"23D8Vr_-Dvs0"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.stats import pearsonr\n","\n","\n","def normalize_predictions(predictions):\n","    mean = np.mean(predictions, axis=0)\n","    std = np.std(predictions, axis=0) + 1e-8  # Avoid division by zero\n","    return (predictions - mean) / std\n","def scale_predictions(predictions):\n","    min_val = np.min(predictions, axis=0)\n","    max_val = np.max(predictions, axis=0)\n","    return (predictions - min_val) / (max_val - min_val)\n","def smooth_predictions(predictions, window_size=5):\n","    smoothed = np.zeros_like(predictions)\n","    for i in range(predictions.shape[1]):  # Loop through each DOF\n","        smoothed[:, i] = np.convolve(predictions[:, i], np.ones(window_size)/window_size, mode='same')\n","        smoothed[:, i] = np.convolve(predictions[:, i], np.ones(window_size)/window_size, mode='same')\n","    return smoothed\n","\n","def evaluate_and_plot(model, updated_weights, query_x, query_y, apply_smoothing=True, apply_normalization=True,apply_Scale=True):\n","    \"\"\"\n","    Evaluate the model for each DOF, calculate CC, and plot actual vs. normalized or smoothed predictions.\n","\n","    Parameters:\n","    - model: The trained model.\n","    - updated_weights: Fine-tuned weights for the task.\n","    - query_x: Input features for the query set.\n","    - query_y: Ground truth outputs for the query set.\n","    - apply_smoothing: Whether to smooth predictions.\n","    - apply_normalization: Whether to normalize predictions.\n","\n","    Returns:\n","    - correlations: List of CC values for each DOF.\n","    - avg_correlation: Average CC across all DOFs.\n","    \"\"\"\n","    # Predict using the updated weights\n","    query_predictions = model(query_x, training=True)\n","\n","    # Normalize or smooth predictions\n","\n","    if apply_smoothing:\n","        query_predictions = smooth_predictions(query_predictions)\n","    if apply_normalization:\n","        query_predictions = normalize_predictions(query_predictions)\n","    if apply_Scale:\n","        query_predictions = scale_predictions(query_predictions)\n","\n","    # Initialize lists for storing CC values\n","    correlations = []\n","\n","    # Loop through each DOF (dimension)\n","    for i in range(query_y.shape[1]):\n","        # Calculate Pearson Correlation Coefficient\n","        cc_value, _ = pearsonr(query_y[:, i], query_predictions[:, i])\n","        correlations.append(cc_value)\n","\n","        # Plot Actual vs Predicted for the current DOF\n","        plt.figure(figsize=(10, 6))\n","        plt.plot(query_y[:, i], label='Actual', color='blue')\n","        plt.plot(query_predictions[:, i], label='Predicted', color='red')\n","\n","        # Add title and labels\n","        plt.title(f'Degree of Freedom {i + 1}: CC = {cc_value:.2f}')\n","        plt.xlabel('Time Steps')\n","        plt.ylabel('Values')\n","        plt.legend()\n","        plt.show()\n","\n","    # Compute the average CC across all DOFs\n","    avg_correlation = np.mean(correlations)\n","    print(f\"Average Pearson Correlation Coefficient: {avg_correlation:.2f}\")\n","\n","    return correlations, avg_correlation\n","\n","def evaluate_ensemble(ensemble_predictions, query_y, apply_smoothing=True, apply_normalization=True, apply_scale=True):\n","    \"\"\"\n","    Evaluate ensemble predictions and plot results.\n","\n","    Parameters:\n","    - ensemble_predictions: Predictions from the ensemble.\n","    - query_y: Ground truth for the query set.\n","    - apply_smoothing: Whether to smooth predictions.\n","    - apply_normalization: Whether to normalize predictions.\n","    - apply_scale: Whether to scale predictions.\n","\n","    Returns:\n","    - correlations: List of CC values for each DOF.\n","    - avg_correlation: Average CC across all DOFs.\n","    \"\"\"\n","    # Process predictions\n","    if apply_smoothing:\n","        ensemble_predictions = smooth_predictions(ensemble_predictions)\n","    if apply_normalization:\n","        ensemble_predictions = normalize_predictions(ensemble_predictions)\n","    if apply_scale:\n","        ensemble_predictions = scale_predictions(ensemble_predictions)\n","\n","    # Initialize lists for storing CC values\n","    correlations = []\n","\n","    # Loop through each DOF (dimension)\n","    for i in range(query_y.shape[1]):\n","        # Calculate Pearson Correlation Coefficient\n","        cc_value, _ = pearsonr(query_y[:, i], ensemble_predictions[:, i])\n","        correlations.append(cc_value)\n","\n","        # Plot Actual vs Predicted for the current DOF\n","        plt.figure(figsize=(10, 6))\n","        plt.plot(query_y[:, i], label='Actual', color='blue')\n","        plt.plot(ensemble_predictions[:, i], label='Predicted', color='red')\n","\n","        # Add title and labels\n","        plt.title(f'Degree of Freedom {i + 1}: CC = {cc_value:.2f}')\n","        plt.xlabel('Time Steps')\n","        plt.ylabel('Values')\n","        plt.legend()\n","        plt.show()\n","\n","    # Compute the average CC across all DOFs\n","    avg_correlation = np.mean(correlations)\n","    print(f\"Average Pearson Correlation Coefficient: {avg_correlation:.2f}\")\n","\n","    return correlations, avg_correlation\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xvSyr83aRWRk"},"source":["## **Methods for saving the data**"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1738650447685,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"},"user_tz":-240},"id":"aVoyRDGSavDh"},"outputs":[],"source":["# Save data in Excel file\n","def savedata(i, correlations):\n","    \"\"\"\n","    Save the correlations data in an Excel file at the specified row.\n","\n","    Parameters:\n","    - i: Integer representing the subject number (e.g., 5 for row name 'S5').\n","    - correlations: List of correlation values to save in the row.\n","    \"\"\"\n","    import openpyxl\n","    from google.colab import drive\n","\n","    # Mount Google Drive\n","    drive.mount('/content/drive')\n","\n","    # Path to the workbook\n","    workbook_path = '/content/drive/My Drive/Colab Notebooks/dataset/Ninapro MAML+weighted ensemble_CNN-LSTM + extra features Subject Independant.xlsx'\n","\n","    # Load the existing workbook\n","    wb = openpyxl.load_workbook(workbook_path)\n","\n","    # Select the active sheet\n","    sheet = wb.active\n","\n","    # Dynamically set the row name based on 'i'\n","    row_name = f'S{i}'  # Example: 'S5' if i=5\n","\n","    # Find the row with the specified row name\n","    target_row = None\n","    for row in sheet.iter_rows(min_row=1, max_row=sheet.max_row):\n","        if row[0].value == row_name:\n","            target_row = row[0].row\n","            break\n","\n","    if target_row is None:\n","        print(f\"Error: Row with name '{row_name}' not found in the sheet.\")\n","        return\n","\n","    # Write the data to the found row, starting from the second column\n","    for col, value in enumerate(correlations, start=2):  # Start at column 2 to skip the first column for the label\n","        sheet.cell(row=target_row, column=col, value=value)\n","\n","    # Save the workbook\n","    wb.save(workbook_path)\n","    print(f\"Data successfully saved for row '{row_name}'.\")\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1738650447685,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"},"user_tz":-240},"id":"DRS9fwzrEBz8"},"outputs":[],"source":["import random\n","def process_subject(test_subject, ensemble, base_path,train_subjects,fine_tune_subjects=5):\n","    \"\"\"\n","    Process a single test subject:\n","    - Fine-tune the ensemble on the support set of the test subject.\n","    - Combine predictions from the ensemble.\n","    - Evaluate and visualize results.\n","    - Return correlations and average correlation.\n","\n","    Parameters:\n","    - test_subject: Subject to be tested (integer).\n","    - ensemble: List of models in the ensemble.\n","    - base_path: Path to the dataset.\n","\n","    Returns:\n","    - correlations: List of correlation coefficients for each DOF.\n","    - avg_correlation: Average correlation across all DOFs.\n","    \"\"\"\n","    print(f\"Processing Test Subject: S{test_subject}\")\n","\n","    # Load test subject data\n","    test_data = loadmat(f\"{base_path}/S{test_subject}_E1.mat\")[\"Data\"]\n","\n","    # Debugging: Check original dataset shape\n","    print(f\"ðŸš€ Debug: Test Subject {test_subject} Original Data Shape = {test_data.shape}\")\n","\n","    # DO NOT SPLIT TEST SUBJECT INTO SUPPORT/QUERY!\n","    Z = test_data[:, :96]  # Input features\n","    X = test_data[:, 96:118]  # DOFs (outputs)\n","\n","    # Reshape for model input\n","    sequence_length = 15\n","    n_sequences = len(Z) - sequence_length + 1\n","\n","    query_x = np.zeros((n_sequences, sequence_length, 96))\n","    query_y = np.zeros((n_sequences, X.shape[1]))\n","\n","    for j in range(n_sequences):\n","        query_x[j] = Z[j:j + sequence_length]\n","        query_y[j] = X[j + sequence_length - 1]\n","\n","    # Debugging: Verify dataset is NOT being shrunk\n","    print(f\"ðŸš€ Debug: Using Full Test Data - Query X = {query_x.shape}, Query Y = {query_y.shape}\")\n","\n","      # Choose a subset of subjects for fine-tuning\n","    fine_tune_subjects_list = random.sample([s for s in train_subjects if s != test_subject], fine_tune_subjects)\n","    print(f\"ðŸš€ Fine-tuning with {fine_tune_subjects} subjects: {fine_tune_subjects_list}\")\n","\n","    support_x_list = []\n","    support_y_list = []\n","\n","    for subject in fine_tune_subjects_list:\n","        train_data = loadmat(f\"{base_path}/S{subject}_E1.mat\")[\"Data\"]\n","        support_x, support_y, _, _ = split_task_data(train_data)  # Train subjects only\n","        support_x_list.append(support_x)\n","        support_y_list.append(support_y)\n","\n","    support_x = np.concatenate(support_x_list, axis=0)\n","    support_y = np.concatenate(support_y_list, axis=0)\n","\n","    print(f\"ðŸš€ Debug: Training Support Dataset - Support X = {support_x.shape}, Support Y = {support_y.shape}\")\n","\n","    # Fine-tune ensemble using reduced support dataset\n","    fine_tuned_weights = fine_tune_ensemble(\n","        ensemble,\n","        support_x,\n","        support_y,\n","        loss_fn=tf.keras.losses.MeanSquaredError(),\n","        inner_lr=0.01,\n","        fine_tune_steps=1\n","    )\n","\n","    # Combine predictions using performance-weighted averaging\n","    ensemble_predictions = ensemble_predict_weighted_performance(\n","        ensemble, fine_tuned_weights, query_x, query_y\n","    )\n","\n","    # Evaluate and visualize results\n","    print(f\"Debug: query_predictions shape: {ensemble_predictions.shape}, query_y shape: {query_y.shape}\")\n","\n","    correlations, avg_correlation = evaluate_ensemble(\n","        ensemble_predictions,\n","        query_y,\n","        apply_smoothing=True,  # Enable smoothing\n","        apply_normalization=False,  # Disable normalization if not needed\n","        apply_scale=True            # Enable scaling\n","    )\n","\n","    # Print results\n","    print(f\"Correlation Coefficient for each DOF (S{test_subject}): {correlations}\")\n","    print(f\"Average Correlation Coefficient (S{test_subject}): {avg_correlation:.2f}\")\n","    print(\"-\" * 50)\n","\n","    return correlations, avg_correlation\n"]},{"cell_type":"markdown","metadata":{"id":"_FOAcRu8eydX"},"source":["### **Main method**"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1BYvexpPWuTvk1mbM4lZCwv96TdpzjxIB"},"id":"RLnfXIFIeyIz","outputId":"6d11f4d1-8339-403e-8b11-54e297276ec1","executionInfo":{"status":"error","timestamp":1738652013301,"user_tz":-240,"elapsed":1565622,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# if __name__ == \"__main__\":\n","#     base_path = '/content/drive/My Drive/Colab Notebooks/dataset wstimi'\n","#     all_subjects = list(range(1, 41))  # Subjects 1 to 40\n","\n","#     for test_subject in all_subjects:\n","#         print(f\"Starting Processing for Test Subject: S{test_subject}\")\n","\n","#         # Exclude the test subject from training subjects\n","#         train_subjects = [s for s in all_subjects if s != test_subject]\n","\n","#         # Prepare tasks for training\n","#         tasks = prepare_tasks(train_subjects, base_path)\n","\n","#         # Train the ensemble of models\n","#         num_models = 5  # Number of models in the ensemble\n","#         ensemble = maml_train_ensemble(num_models, tasks, epochs=50, inner_lr=0.01, meta_lr=0.001)\n","\n","#         # Process the current test subject\n","#         correlations, avg_correlation = process_subject(test_subject, ensemble, base_path)\n","\n","#         # Save the results\n","#         savedata(test_subject, correlations)\n","\n","import os\n","\n","if __name__ == \"__main__\":\n","    base_path = '/content/drive/My Drive/Colab Notebooks/dataset wstimi'\n","    all_subjects = list(range(1, 41))  # Subjects 1 to 40\n","\n","    # Start resuming from test subject 10\n","    start_subject = 1\n","\n","    for test_subject in all_subjects:\n","        # Skip subjects that were already processed\n","        if test_subject < start_subject:\n","            continue  # Skip to the starting subject\n","\n","        print(f\"Starting Processing for Test Subject: S{test_subject}\")\n","\n","        # Ensure all other subjects (1â€“40) except the current one are in the training set\n","        train_subjects = [s for s in all_subjects if s != test_subject]\n","\n","        # Prepare tasks for training\n","        tasks = prepare_tasks(train_subjects, base_path)\n","\n","        # Train the ensemble of models\n","        num_models = 1  # Number of models in the ensemble\n","        ensemble = maml_train_ensemble(num_models, tasks, epochs=70, inner_lr=0.01, meta_lr=0.001)\n","\n","        # Process the current test subject\n","        correlations, avg_correlation = process_subject(test_subject, ensemble, base_path,train_subjects,5)\n","\n","        # Save the results\n","        # savedata(test_subject, correlations)\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyMPSSwBenywUbORWEPLFMb0"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}