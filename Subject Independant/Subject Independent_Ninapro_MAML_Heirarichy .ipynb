{"cells":[{"cell_type":"markdown","metadata":{"id":"FVw_sHiQav9Y"},"source":["### **Connecting with Drive**"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2806,"status":"ok","timestamp":1737389471273,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"},"user_tz":-240},"id":"3fuk29ccavxQ","outputId":"24653f3e-a15f-4b64-842c-4ca3f3a78292"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["#Reading the training data Subject\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"OWU0BwfKaqMa"},"source":["### **Importing All needed libraries**"]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1737389471273,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"},"user_tz":-240},"id":"zvMx9ro8ZntG","outputId":"891ff670-8934-4d3d-ae81-f52644dd0197"},"outputs":[{"output_type":"stream","name":"stdout","text":["ERROR:tensorflow:An interactive session is already active. This can cause out-of-memory errors or some other unexpected errors (due to the unpredictable timing of garbage collection) in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s). Please use `tf.Session()` if you intend to productionize.\n"]}],"source":["#Importing all needed libraries\n","import pandas as pd\n","import numpy as np #Matric math\n","import tensorflow as tf #ML\n","from tensorflow.python.framework import ops\n","from random import randint\n","from numpy import array\n","from numpy import argmax\n","import keras.backend as K\n","from tensorflow.keras import models\n","from numpy import array_equal\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import LSTM, Bidirectional\n","from tensorflow.keras.layers import Dense, Flatten\n","from tensorflow.keras import Input\n","from tensorflow.keras.layers import TimeDistributed\n","from tensorflow.keras.layers import RepeatVector\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.utils import plot_model\n","import matplotlib.pyplot as plt\n","import sys\n","import os\n","from scipy.io import loadmat\n","from scipy.io import loadmat\n","\n","# sys.path.append(os.path.abspath(\"/Users/henda/anaconda3/Lib/site-packages\"))\n","# from rnn_utils import *\n","# from public_tests import *\n","ops.reset_default_graph()\n","\n","tf.compat.v1.reset_default_graph() #Clearning cache\n","sess=tf.compat.v1.InteractiveSession()\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f27nYeB4a4F9"},"source":["### **Define Task**"]},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1737389471273,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"},"user_tz":-240},"id":"PlWYChOwbtQ6"},"outputs":[],"source":["# import numpy as np\n","# import tensorflow as tf\n","# from tensorflow.keras import layers, Model, Input\n","# from scipy.io import loadmat\n","# import matplotlib.pyplot as plt\n","# from scipy.stats import pearsonr\n","# import openpyxl\n","\n","# # -------------------- Data Preparation --------------------\n","\n","# def split_task_data_hierarchy(data, sequence_length=15, n_features=60, split_ratio=0.8):\n","#     Z = data[:, 0:60]  # Columns 0 to 59 → 60 columns\n","#     X = data[:, 60:82] # Columns 60 to 81 → 22 columns\n","#     print (Z.shape)\n","#     print(X.shape)\n","#     n_sequences = len(Z) - sequence_length + 1\n","\n","#     inputs = np.zeros((n_sequences, sequence_length, n_features))\n","#     outputs = np.zeros((n_sequences, X.shape[1]))\n","\n","#     for j in range(n_sequences):\n","#         inputs[j] = Z[j:j + sequence_length]\n","#         outputs[j] = X[j + sequence_length - 1]\n","\n","#     split_index = int(len(inputs) * split_ratio)\n","#     support_x, query_x = inputs[:split_index], inputs[split_index:]\n","#     support_y, query_y = outputs[:split_index], outputs[split_index:]\n","\n","#     level1_dofs = [0, 3, 4, 5, 7, 8, 11, 12, 15, 16, 20, 21]\n","#     level2_dofs = [i for i in range(22) if i not in level1_dofs]\n","\n","#     support_y_level1, support_y_level2 = support_y[:, level1_dofs], support_y[:, level2_dofs]\n","#     query_y_level1, query_y_level2 = query_y[:, level1_dofs], query_y[:, level2_dofs]\n","\n","#     return support_x, (support_y_level1, support_y_level2), query_x, (query_y_level1, query_y_level2)\n","\n","\n","# def prepare_tasks(subjects, base_path, sequence_length=15, n_features=60):\n","#     tasks = []\n","#     for subject in subjects:\n","#         data_path = f\"{base_path}/S{subject}_E1.mat\"\n","#         subject_data = loadmat(data_path)[\"Data\"]\n","#         support_x, (support_y_full1, support_y_full2), query_x, (query_y_full1, query_y_full2) = split_task_data_hierarchy(subject_data, sequence_length, n_features)\n","#         tasks.append((support_x, support_y_full1, support_y_full2, query_x, query_y_full1, query_y_full2))\n","#     return tasks"]},{"cell_type":"markdown","metadata":{"id":"3O4YAmlwccNi"},"source":["### **build model**"]},{"cell_type":"code","execution_count":49,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1737389471273,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"},"user_tz":-240},"id":"AW0G3S9cb1G7"},"outputs":[],"source":["# import numpy as np\n","# import tensorflow as tf\n","# from tensorflow.keras import layers, Model, Input\n","\n","# # -------------------- Enhanced Model Building --------------------\n","\n","# def build_model_level1(input_features=60, timesteps=15, output_dim=12):\n","#     inputs = Input(shape=(timesteps, input_features))\n","\n","#     # CNN Layers\n","#     x = layers.Conv1D(32, kernel_size=3, padding='same', activation='relu')(inputs)\n","#     x = layers.BatchNormalization()(x)\n","#     x = layers.MaxPooling1D(pool_size=2, padding='same')(x)\n","\n","#     x = layers.Conv1D(64, kernel_size=3, padding='same', activation='relu')(inputs)\n","#     x = layers.BatchNormalization()(x)\n","#     x = layers.MaxPooling1D(pool_size=2, padding='same')(x)\n","\n","#     x = layers.Conv1D(128, kernel_size=3, padding='same', activation='relu')(x)\n","#     x = layers.BatchNormalization()(x)\n","#     x = layers.MaxPooling1D(pool_size=2, padding='same')(x)\n","\n","#     # LSTM Layers\n","#     x = layers.Bidirectional(layers.LSTM(500, return_sequences=True))(x)\n","#     x = layers.Dropout(0.3)(x)\n","#     x = layers.Bidirectional(layers.LSTM(500))(x)\n","#     x = layers.Dropout(0.3)(x)\n","\n","#     outputs = layers.Dense(output_dim, activation='linear')(x)\n","#     return Model(inputs, outputs)\n","\n","\n","# def build_model_level2(input_features=72, timesteps=15, output_dim=10):\n","#     inputs = Input(shape=(timesteps, input_features))\n","\n","#     # Enhanced CNN Layers\n","#     x = layers.Conv1D(64, kernel_size=3, padding='same', activation='relu')(inputs)\n","#     x = layers.BatchNormalization()(x)\n","#     x = layers.MaxPooling1D(pool_size=2, padding='same')(x)\n","\n","#     x = layers.Conv1D(128, kernel_size=3, padding='same', activation='relu')(x)\n","#     x = layers.BatchNormalization()(x)\n","#     x = layers.MaxPooling1D(pool_size=2, padding='same')(x)\n","\n","#     x = layers.Conv1D(256, kernel_size=3, padding='same', activation='relu')(x)\n","#     x = layers.BatchNormalization()(x)\n","#     x = layers.MaxPooling1D(pool_size=2, padding='same')(x)\n","\n","#     # LSTM Layers\n","#     x = layers.Bidirectional(layers.LSTM(1000, return_sequences=True))(x)\n","#     x = layers.Dropout(0.4)(x)\n","#     x = layers.Bidirectional(layers.LSTM(800))(x)\n","#     x = layers.Dropout(0.4)(x)\n","\n","#     outputs = layers.Dense(output_dim, activation='linear')(x)\n","#     return Model(inputs, outputs)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Xko9re4oc81e"},"source":["### **Training Model framework**"]},{"cell_type":"code","execution_count":50,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1737389471273,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"},"user_tz":-240},"id":"rKEJpS9zGCvJ"},"outputs":[],"source":["\n","\n","# # import numpy s np\n","# import tensorflow as tf\n","# from tensorflow.keras import layers, Model, Input\n","# from scipy.io import loadmat\n","# import matplotlib.pyplot as plt\n","# from scipy.stats import pearsonr\n","# import openpyxl\n","\n","# # -------------------- MAML Joint Inner Loop --------------------\n","\n","# def joint_inner_loop(model_level1, model_level2, support_x, support_y_level1, support_y_level2, loss_fn, inner_lr):\n","#     with tf.GradientTape(persistent=True) as tape:\n","#         predictions1 = model_level1(support_x, training=True)\n","#         predictions1_expanded = tf.repeat(tf.expand_dims(predictions1, axis=1), repeats=support_x.shape[1], axis=1)\n","#         support_x_level2 = tf.concat([support_x, predictions1_expanded], axis=-1)\n","\n","#         predictions2 = model_level2(support_x_level2, training=True)\n","\n","#         loss1 = loss_fn(support_y_level1, predictions1)\n","#         loss2 = loss_fn(support_y_level2, predictions2)\n","#         total_loss = loss1 + loss2\n","\n","#     grads1 = tape.gradient(total_loss, model_level1.trainable_variables)\n","#     grads2 = tape.gradient(total_loss, model_level2.trainable_variables)\n","#     return grads1, grads2, total_loss\n","\n","# # -------------------- Ensemble MAML Joint Training --------------------\n","\n","# def maml_joint_train_ensemble(num_models, tasks, epochs, inner_lr, meta_lr, model_builder_level1, model_builder_level2):\n","#     ensemble_level1, ensemble_level2 = [], []\n","#     for i in range(num_models):\n","#         model_level1 = model_builder_level1()\n","#         model_level2 = model_builder_level2()\n","#         optimizer1 = tf.keras.optimizers.Adam(meta_lr)\n","#         optimizer2 = tf.keras.optimizers.Adam(meta_lr)\n","#         loss_fn = tf.keras.losses.MeanSquaredError()\n","\n","#         for epoch in range(epochs):\n","#             meta_grads1 = [tf.zeros_like(var) for var in model_level1.trainable_variables]\n","#             meta_grads2 = [tf.zeros_like(var) for var in model_level2.trainable_variables]\n","\n","#             for support_x, support_y_level1, support_y_level2, query_x, query_y_level1, query_y_level2 in tasks:\n","#                 grads1, grads2, _ = joint_inner_loop(model_level1, model_level2, support_x, support_y_level1, support_y_level2, loss_fn, inner_lr)\n","#                 meta_grads1 = [m + g for m, g in zip(meta_grads1, grads1)]\n","#                 meta_grads2 = [m + g for m, g in zip(meta_grads2, grads2)]\n","\n","#             meta_grads1 = [g / len(tasks) for g in meta_grads1]\n","#             meta_grads2 = [g / len(tasks) for g in meta_grads2]\n","#             optimizer1.apply_gradients(zip(meta_grads1, model_level1.trainable_variables))\n","#             optimizer2.apply_gradients(zip(meta_grads2, model_level2.trainable_variables))\n","\n","#         ensemble_level1.append(model_level1)\n","#         ensemble_level2.append(model_level2)\n","#     return ensemble_level1, ensemble_level2\n","\n","# # -------------------- Weighted Ensemble Prediction --------------------\n","\n","# def weighted_ensemble_predict(ensemble, query_x, true_y):\n","#     predictions = [model.predict(query_x) for model in ensemble]\n","#     errors = [np.sqrt(np.mean((true_y - pred) ** 2)) for pred in predictions]\n","#     weights = [1 / (err + 1e-8) for err in errors]\n","#     weights /= np.sum(weights)\n","#     weighted_predictions = np.average(predictions, axis=0, weights=weights)\n","#     return weighted_predictions\n"]},{"cell_type":"markdown","metadata":{"id":"X4ydt6dqcBiY"},"source":["## **Evaluation Framework**"]},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1737389471273,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"},"user_tz":-240},"id":"VXVG25jVcAS0"},"outputs":[],"source":["# # -------------------- Prediction and Evaluation --------------------\n","\n","# def ensemble_predict(ensemble, query_x):\n","#     predictions = [model(query_x, training=False) for model in ensemble]\n","#     return np.mean(predictions, axis=0)\n","\n","# import matplotlib.pyplot as plt\n","# from scipy.stats import pearsonr\n","\n","# def evaluate_model(predictions, query_y):\n","#     # Handle tuple case for hierarchical output\n","#     if isinstance(query_y, tuple):\n","#         query_y = np.concatenate(query_y, axis=-1)\n","\n","#     correlations = []\n","\n","#     # Plot and compute CC for each DOF\n","#     for i in range(query_y.shape[1]):\n","#         cc_value, _ = pearsonr(query_y[:, i], predictions[:, i])\n","#         correlations.append(cc_value)\n","\n","#         # Plot actual vs predicted for each DOF\n","#         plt.figure(figsize=(10, 5))\n","#         plt.plot(query_y[:, i], label='Actual', color='blue')\n","#         plt.plot(predictions[:, i], label='Predicted', color='red')\n","#         plt.title(f'Degree of Freedom {i + 1}: Pearson CC = {cc_value:.2f}')\n","#         plt.xlabel('Time Steps')\n","#         plt.ylabel('Value')\n","#         plt.legend()\n","#         plt.show()\n","\n","#         print(f\"DOF {i + 1}: Pearson Correlation Coefficient = {cc_value:.4f}\")\n","\n","#     avg_correlation = np.mean(correlations)\n","#     print(f\"\\nAverage Pearson Correlation Coefficient: {avg_correlation:.4f}\")\n","#     return correlations, avg_correlation\n","\n","\n"]},{"cell_type":"code","execution_count":52,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1737389471274,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"},"user_tz":-240},"id":"OaLDwHFWMJhM"},"outputs":[],"source":["# def savedata(test_subject, correlations):\n","#     from google.colab import drive\n","#     import openpyxl\n","\n","#     # Mount Google Drive only if not already mounted\n","#     if not os.path.ismount('/content/drive'):\n","#         drive.mount('/content/drive')\n","\n","#     # Define the workbook path\n","#     workbook_path = '/content/drive/My Drive/Colab Notebooks/dataset/Ninapro MAML-Hirarichal Subject Independant.xlsx'\n","\n","#     # Load the workbook\n","#     wb = openpyxl.load_workbook(workbook_path)\n","#     sheet = wb.active\n","\n","#     # Locate the row for the test subject\n","#     row_name = f'S{test_subject}'\n","#     target_row = None\n","#     for row in sheet.iter_rows(min_row=1, max_row=sheet.max_row):\n","#         if row[0].value == row_name:\n","#             target_row = row[0].row\n","#             break\n","\n","#     if target_row is None:\n","#         raise ValueError(f\"Row '{row_name}' not found.\")\n","\n","#     # Save correlation values\n","#     for col, value in enumerate(correlations, start=2):\n","#         sheet.cell(row=target_row, column=col, value=value)\n","\n","#     wb.save(workbook_path)\n","#     print(f\"Results saved for Subject {test_subject}.\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_FOAcRu8eydX"},"source":["### **Main method**"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"RLnfXIFIeyIz","executionInfo":{"status":"ok","timestamp":1737389471274,"user_tz":-240,"elapsed":8,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"}}},"outputs":[],"source":["# # -------------------- DOF for each layer --------------------\n","\n","# level1_dofs = [0, 3, 4, 5, 7, 8, 11, 12, 15, 16, 20, 21]\n","# level2_dofs = [i for i in range(22) if i not in level1_dofs]\n","\n","\n"]},{"cell_type":"code","source":["# if __name__ == \"__main__\":\n","#     base_path = '/content/drive/My Drive/Colab Notebooks/dataset wstimi'\n","#     all_subjects = list(range(1, 41))\n","\n","#     for test_subject in all_subjects:\n","#         print(f\"Processing Test Subject: S{test_subject}\")\n","#         train_subjects = [s for s in all_subjects if s != test_subject]\n","#         tasks = prepare_tasks(train_subjects, base_path)\n","\n","#         ensemble_level1, ensemble_level2 = maml_joint_train_ensemble(5, tasks, 50, 0.01, 0.001, build_model_level1, build_model_level2)\n","\n","#         test_data = loadmat(f\"{base_path}/S{test_subject}_E1.mat\")[\"Data\"]\n","#         support_x, (support_y_level1, support_y_level2), query_x, (query_y_level1, query_y_level2) = split_task_data_hierarchy(test_data)\n","\n","#         predictions_level1 = weighted_ensemble_predict(ensemble_level1, query_x, query_y_level1)\n","#         predictions1_expanded = tf.repeat(tf.expand_dims(predictions_level1, axis=1), repeats=query_x.shape[1], axis=1)\n","#         query_x_level2 = tf.concat([query_x, predictions1_expanded], axis=-1)\n","\n","#         predictions_level2 = weighted_ensemble_predict(ensemble_level2, query_x_level2, query_y_level2)\n","\n","#         combined_predictions = np.zeros((predictions_level1.shape[0], 22))\n","#         for idx, dof in enumerate(level1_dofs): combined_predictions[:, dof] = predictions_level1[:, idx]\n","#         for idx, dof in enumerate(level2_dofs): combined_predictions[:, dof] = predictions_level2[:, idx]\n","\n","#         correlations, avg_correlation = evaluate_model(combined_predictions, np.concatenate([query_y_level1, query_y_level2], axis=-1))\n","#         savedata(test_subject, correlations)\n"],"metadata":{"id":"EaIIg6UBgBC5","executionInfo":{"status":"ok","timestamp":1737389471274,"user_tz":-240,"elapsed":8,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"}}},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":["### **Reading Data and preparing it**"],"metadata":{"id":"rdhvC7MBULNa"}},{"cell_type":"code","source":["# # Split data into support and query sets\n","# def split_task_data(data, sequence_length=15, n_features=60, split_ratio=0.8):\n","#     # X = data[:, 36:58]\n","#     # Z = data[:, 58:82]\n","#     Z = data[:, 0:60]  # Columns 0 to 59 → 60 columns\n","#     X = data[:, 60:82] # Columns 60 to 81 → 22 columns\n","#     n_sequences = len(Z) - sequence_length + 1\n","\n","#     inputs = np.zeros((n_sequences, sequence_length, n_features))\n","#     outputs = np.zeros((n_sequences, X.shape[1]))\n","\n","#     for j in range(n_sequences):\n","#         inputs[j] = Z[j:j + sequence_length]\n","#         outputs[j] = X[j + sequence_length - 1]\n","\n","#     # Split into support and query sets\n","#     split_index = int(len(inputs) * split_ratio)\n","#     support_x, query_x = inputs[:split_index], inputs[split_index:]\n","#     support_y, query_y = outputs[:split_index], outputs[split_index:]\n","#     return support_x, support_y, query_x, query_y\n","\n","# # Prepare tasks for MAML\n","# def prepare_tasks(subjects, base_path, sequence_length=15, n_features=60):\n","#     tasks = []\n","#     for subject in subjects:\n","#         data_path = f\"{base_path}/S{subject}_E1.mat\"\n","#         subject_data = loadmat(data_path)[\"Data\"]\n","#         print(f\"Shape of subject {subject}: {subject_data.shape}\")\n","#         support_x, support_y, query_x, query_y = split_task_data(subject_data, sequence_length, n_features)\n","#         print(f\"Shape of support_x for subject {subject}: {support_x.shape}\")\n","#         print(f\"Shape of support_y for subject {subject}: {support_y.shape}\")\n","#         print(f\"Shape of query_x for subject {subject}: {query_x.shape}\")\n","#         print(f\"Shape of query_y for subject {subject}: {query_y.shape}\")\n","#         tasks.append((support_x, support_y, query_x, query_y))\n","\n","#     return tasks\n"],"metadata":{"id":"o5kakZIoUFEo","executionInfo":{"status":"ok","timestamp":1737389471274,"user_tz":-240,"elapsed":8,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["# import numpy as np\n","# import matplotlib.pyplot as plt\n","# from scipy.stats import pearsonr\n","\n","\n","# def normalize_predictions(predictions):\n","#     mean = np.mean(predictions, axis=0)\n","#     std = np.std(predictions, axis=0) + 1e-8  # Avoid division by zero\n","#     return (predictions - mean) / std\n","# def scale_predictions(predictions):\n","#     min_val = np.min(predictions, axis=0)\n","#     max_val = np.max(predictions, axis=0)\n","#     return (predictions - min_val) / (max_val - min_val)\n","# def smooth_predictions(predictions, window_size=5):\n","#     smoothed = np.zeros_like(predictions)\n","#     for i in range(predictions.shape[1]):  # Loop through each DOF\n","#         smoothed[:, i] = np.convolve(predictions[:, i], np.ones(window_size)/window_size, mode='same')\n","#         smoothed[:, i] = np.convolve(predictions[:, i], np.ones(window_size)/window_size, mode='same')\n","#     return smoothed\n","\n","# def evaluate_and_plot(model, updated_weights, query_x, query_y, apply_smoothing=True, apply_normalization=True,apply_Scale=True):\n","#     \"\"\"\n","#     Evaluate the model for each DOF, calculate CC, and plot actual vs. normalized or smoothed predictions.\n","\n","#     Parameters:\n","#     - model: The trained model.\n","#     - updated_weights: Fine-tuned weights for the task.\n","#     - query_x: Input features for the query set.\n","#     - query_y: Ground truth outputs for the query set.\n","#     - apply_smoothing: Whether to smooth predictions.\n","#     - apply_normalization: Whether to normalize predictions.\n","\n","#     Returns:\n","#     - correlations: List of CC values for each DOF.\n","#     - avg_correlation: Average CC across all DOFs.\n","#     \"\"\"\n","#     # Predict using the updated weights\n","#     query_predictions = model(query_x, training=True)\n","\n","#     # Normalize or smooth predictions\n","\n","#     if apply_smoothing:\n","#         query_predictions = smooth_predictions(query_predictions)\n","#     if apply_normalization:\n","#         query_predictions = normalize_predictions(query_predictions)\n","#     if apply_Scale:\n","#         query_predictions = scale_predictions(query_predictions)\n","\n","#     # Initialize lists for storing CC values\n","#     correlations = []\n","\n","#     # Loop through each DOF (dimension)\n","#     for i in range(query_y.shape[1]):\n","#         # Calculate Pearson Correlation Coefficient\n","#         cc_value, _ = pearsonr(query_y[:, i], query_predictions[:, i])\n","#         correlations.append(cc_value)\n","\n","#         # Plot Actual vs Predicted for the current DOF\n","#         plt.figure(figsize=(10, 6))\n","#         plt.plot(query_y[:, i], label='Actual', color='blue')\n","#         plt.plot(query_predictions[:, i], label='Predicted', color='red')\n","\n","#         # Add title and labels\n","#         plt.title(f'Degree of Freedom {i + 1}: CC = {cc_value:.2f}')\n","#         plt.xlabel('Time Steps')\n","#         plt.ylabel('Values')\n","#         plt.legend()\n","#         plt.show()\n","\n","#     # Compute the average CC across all DOFs\n","#     avg_correlation = np.mean(correlations)\n","#     print(f\"Average Pearson Correlation Coefficient: {avg_correlation:.2f}\")\n","\n","#     return correlations, avg_correlation\n","\n","# def evaluate_ensemble(ensemble_predictions, query_y, apply_smoothing=True, apply_normalization=True, apply_scale=True):\n","#     \"\"\"\n","#     Evaluate ensemble predictions and plot results.\n","\n","#     Parameters:\n","#     - ensemble_predictions: Predictions from the ensemble.\n","#     - query_y: Ground truth for the query set.\n","#     - apply_smoothing: Whether to smooth predictions.\n","#     - apply_normalization: Whether to normalize predictions.\n","#     - apply_scale: Whether to scale predictions.\n","\n","#     Returns:\n","#     - correlations: List of CC values for each DOF.\n","#     - avg_correlation: Average CC across all DOFs.\n","#     \"\"\"\n","#     # Process predictions\n","#     if apply_smoothing:\n","#         ensemble_predictions = smooth_predictions(ensemble_predictions)\n","#     if apply_normalization:\n","#         ensemble_predictions = normalize_predictions(ensemble_predictions)\n","#     if apply_scale:\n","#         ensemble_predictions = scale_predictions(ensemble_predictions)\n","\n","#     # Initialize lists for storing CC values\n","#     correlations = []\n","\n","#     # Loop through each DOF (dimension)\n","#     for i in range(query_y.shape[1]):\n","#         # Calculate Pearson Correlation Coefficient\n","#         cc_value, _ = pearsonr(query_y[:, i], ensemble_predictions[:, i])\n","#         correlations.append(cc_value)\n","\n","#         # Plot Actual vs Predicted for the current DOF\n","#         plt.figure(figsize=(10, 6))\n","#         plt.plot(query_y[:, i], label='Actual', color='blue')\n","#         plt.plot(ensemble_predictions[:, i], label='Predicted', color='red')\n","\n","#         # Add title and labels\n","#         plt.title(f'Degree of Freedom {i + 1}: CC = {cc_value:.2f}')\n","#         plt.xlabel('Time Steps')\n","#         plt.ylabel('Values')\n","#         plt.legend()\n","#         plt.show()\n","\n","#     # Compute the average CC across all DOFs\n","#     avg_correlation = np.mean(correlations)\n","#     print(f\"Average Pearson Correlation Coefficient: {avg_correlation:.2f}\")\n","\n","#     return correlations, avg_correlation\n","\n"],"metadata":{"id":"RBLFoFmaUFAE","executionInfo":{"status":"ok","timestamp":1737389471274,"user_tz":-240,"elapsed":8,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["# # Save data in Excel file\n","# def savedata(i, correlations):\n","#     \"\"\"\n","#     Save the correlations data in an Excel file at the specified row.\n","\n","#     Parameters:\n","#     - i: Integer representing the subject number (e.g., 5 for row name 'S5').\n","#     - correlations: List of correlation values to save in the row.\n","#     \"\"\"\n","#     import openpyxl\n","#     from google.colab import drive\n","\n","#     # Mount Google Drive\n","#     drive.mount('/content/drive')\n","\n","#     # Path to the workbook\n","#     workbook_path = '/content/drive/My Drive/Colab Notebooks/dataset/Ninapro MAML-Kinematics model Subject Independant.xlsx'\n","\n","#     # Load the existing workbook\n","#     wb = openpyxl.load_workbook(workbook_path)\n","\n","#     # Select the active sheet\n","#     sheet = wb.active\n","\n","#     # Dynamically set the row name based on 'i'\n","#     row_name = f'S{i}'\n","\n","#     # Find the row with the specified row name\n","#     target_row = None\n","#     for row in sheet.iter_rows(min_row=1, max_row=sheet.max_row):\n","#         if row[0].value == row_name:\n","#             target_row = row[0].row\n","#             break\n","\n","#     if target_row is None:\n","#         print(f\"Error: Row with name '{row_name}' not found in the sheet.\")\n","#         return\n","\n","#     # Write the data to the found row, starting from the second column\n","#     for col, value in enumerate(correlations, start=2):  # Start at column 2 to skip the first column for the label\n","#         sheet.cell(row=target_row, column=col, value=value)\n","\n","#     # Save the workbook\n","#     wb.save(workbook_path)\n","#     print(f\"Data successfully saved for row '{row_name}'.\")\n","\n"],"metadata":{"id":"k0JT9z5nUhKb","executionInfo":{"status":"ok","timestamp":1737389471275,"user_tz":-240,"elapsed":9,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"}}},"execution_count":57,"outputs":[]},{"cell_type":"code","source":["# import tensorflow as tf\n","# from tensorflow.keras import layers, Model, Input\n","# import numpy as np\n","\n","# # -------------------- Model Building with Constraints --------------------\n","\n","# def build_multitask_model_with_constraints(input_features=60, timesteps=15, output_dim=22):\n","#     inputs = Input(shape=(timesteps, input_features))\n","\n","#     # CNN Layers\n","#     x = layers.Conv1D(32, kernel_size=3, activation='relu', padding='same')(inputs)\n","#     x = layers.MaxPooling1D(pool_size=2, padding='same')(x)\n","#     x = layers.Conv1D(64, kernel_size=3, activation='relu', padding='same')(x)\n","#     x = layers.MaxPooling1D(pool_size=2, padding='same')(x)\n","#     x = layers.Conv1D(128, kernel_size=3, activation='relu', padding='same')(x)\n","#     x = layers.MaxPooling1D(pool_size=2, padding='same')(x)\n","\n","#     # LSTM Layers\n","#     x = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(x)\n","#     x = layers.Dropout(0.3)(x)\n","#     x = layers.Bidirectional(layers.LSTM(256))(x)\n","#     x = layers.Dropout(0.3)(x)\n","\n","#     outputs = layers.Dense(output_dim, activation='linear')(x)\n","#     return Model(inputs, outputs)\n","# # -------------------- MAML Training with Constraints --------------------\n","\n","# def maml_train_with_constraints(model, tasks, epochs, inner_lr, meta_lr):\n","#     optimizer = tf.keras.optimizers.Adam(meta_lr)\n","#     loss_fn = tf.keras.losses.MeanSquaredError()\n","\n","#     for epoch in range(epochs):\n","#         for support_x, support_y, query_x, query_y in tasks:\n","#             with tf.GradientTape() as tape:\n","#                 # Inner loop update\n","#                 predictions_support = model(support_x, training=True)\n","#                 loss_support = loss_fn(support_y, predictions_support)\n","#                 grads = tape.gradient(loss_support, model.trainable_variables)\n","#                 updated_weights = [w - inner_lr * g for w, g in zip(model.trainable_variables, grads)]\n","\n","#             # Outer loop update\n","#             with tf.GradientTape() as tape:\n","#                 model.set_weights(updated_weights)\n","#                 predictions_query = model(query_x, training=True)\n","#                 loss_query = loss_fn(query_y, predictions_query)\n","#             grads_meta = tape.gradient(loss_query, model.trainable_variables)\n","#             optimizer.apply_gradients(zip(grads_meta, model.trainable_variables))\n","#         print(f\"Epoch {epoch + 1}/{epochs} completed\")\n","\n","# # -------------------- Ensemble Prediction with Kinematics --------------------\n","\n","# def ensemble_predict_with_kinematics(ensemble, query_x):\n","#     predictions = [model.predict(query_x) for model in ensemble]\n","#     averaged_predictions = np.mean(predictions, axis=0)\n","#     return averaged_predictions\n","\n","\n","\n"],"metadata":{"id":"QdWHuJ_NVCsW","executionInfo":{"status":"ok","timestamp":1737389471275,"user_tz":-240,"elapsed":9,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["# if __name__ == \"__main__\":\n","#     base_path = '/content/drive/My Drive/Colab Notebooks/dataset wstimi'\n","#     all_subjects = list(range(1, 41))\n","\n","#     for test_subject in all_subjects:\n","#         print(f\"Processing Test Subject: S{test_subject}\")\n","\n","#         train_subjects = [s for s in all_subjects if s != test_subject]\n","#         tasks = prepare_tasks(train_subjects, base_path)\n","\n","#         # Train with Constraints and MAML\n","#         ensemble = []\n","#         for i in range(2):  # Number of ensemble models\n","#             print(f\"Training Model {i + 1}/2 with constraints...\")\n","#             model = build_multitask_model_with_constraints()\n","#             maml_train_with_constraints(model, tasks, epochs=50, inner_lr=0.01, meta_lr=0.001)\n","#             ensemble.append(model)\n","\n","#         # Evaluate on Test Subject\n","#         test_data = loadmat(f\"{base_path}/S{test_subject}_E1.mat\")[\"Data\"]\n","#         support_x, support_y, query_x, query_y = split_task_data(test_data)\n","\n","#         # Predict and Adjust with Kinematics\n","#         predictions = ensemble_predict_with_kinematics(ensemble, query_x)\n","\n","#         # Evaluate\n","#         correlations, avg_correlation = evaluate_ensemble(predictions, query_y,apply_smoothing=False, apply_normalization=True, apply_scale=True)\n","#         print(f\"Test Subject {test_subject}: Avg CC = {avg_correlation:.2f}\")\n","#         savedata(test_subject, correlations)\n"],"metadata":{"id":"ZIifqazxUE7r","executionInfo":{"status":"ok","timestamp":1737389471275,"user_tz":-240,"elapsed":8,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["# !pip install -q condacolab\n","# import condacolab\n","# condacolab.install()\n","# !conda install -c opensim_admin opensim"],"metadata":{"id":"plMnfsnxXT3V","executionInfo":{"status":"ok","timestamp":1737389471275,"user_tz":-240,"elapsed":8,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"}}},"execution_count":60,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyPBDVwQThFljLRVjpwh0jeB"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}