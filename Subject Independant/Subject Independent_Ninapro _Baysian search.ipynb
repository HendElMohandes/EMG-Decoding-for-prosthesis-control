{"cells":[{"cell_type":"markdown","metadata":{"id":"FVw_sHiQav9Y"},"source":["### **Connecting with Drive**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2815,"status":"ok","timestamp":1735243497137,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"},"user_tz":-240},"id":"3fuk29ccavxQ","outputId":"119ec34c-9241-434d-fcd5-e3e33c0384c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["#Reading the training data Subject\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2525,"status":"ok","timestamp":1735243499660,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"},"user_tz":-240},"id":"F7kBkLMQ8l0O","outputId":"2958afce-a18b-422c-e66d-8f30c6e842f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: geneticalgorithm in /usr/local/lib/python3.10/dist-packages (1.0.2)\n","Requirement already satisfied: func-timeout in /usr/local/lib/python3.10/dist-packages (from geneticalgorithm) (4.3.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from geneticalgorithm) (1.26.4)\n"]}],"source":["!pip install geneticalgorithm\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2650,"status":"ok","timestamp":1735243502307,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"},"user_tz":-240},"id":"XjAhsGYwgilC","outputId":"a7347bbd-d970-4eb1-e7a5-1f2a049aa323"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.1.0)\n","Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.14.0)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.9.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n","Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.67.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.8)\n","Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n"]}],"source":["!pip install optuna"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RiOof3_jitD5"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from scipy.io import loadmat\n","from scipy.stats import pearsonr\n","from tensorflow.keras import layers, Model, Input\n","from tensorflow.keras.optimizers import Adam\n","import optuna\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","# Augmentation function for EMG signals\n","def augment_emg_signals(data):\n","    noise = np.random.normal(0, 0.1, data.shape)  # Add Gaussian noise\n","    scaling = np.random.uniform(0.8, 1.2, size=(data.shape[0], 1, data.shape[2]))\n","    return data * scaling + noise\n","\n","# Define SimpleGNNLayer\n","class SimpleGNNLayer(layers.Layer):\n","    def __init__(self, num_features, **kwargs):\n","        super(SimpleGNNLayer, self).__init__(**kwargs)\n","        self.num_features = num_features\n","\n","    def build(self, input_shape):\n","        self.adjacency_matrix = self.add_weight(\n","            shape=(input_shape[-1], self.num_features),\n","            initializer=\"random_normal\",\n","            trainable=True,\n","            name=\"adjacency_matrix\"\n","        )\n","\n","    def call(self, inputs):\n","        return tf.einsum('bti,ij->btj', inputs, self.adjacency_matrix)\n","\n","# Define Domain-Specific Normalization\n","class DomainSpecificNormalization(layers.Layer):\n","    def __init__(self, **kwargs):\n","        super(DomainSpecificNormalization, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.gamma = self.add_weight(\n","            shape=(1, 1, input_shape[-1]),\n","            initializer=\"ones\",\n","            trainable=True,\n","            name=\"gamma\"\n","        )\n","        self.beta = self.add_weight(\n","            shape=(1, 1, input_shape[-1]),\n","            initializer=\"zeros\",\n","            trainable=True,\n","            name=\"beta\"\n","        )\n","\n","    def call(self, inputs):\n","        mean = tf.reduce_mean(inputs, axis=(1, 2), keepdims=True)\n","        std = tf.math.reduce_std(inputs, axis=(1, 2), keepdims=True)\n","        normalized = (inputs - mean) / (std + tf.keras.backend.epsilon())\n","        return self.gamma * normalized + self.beta\n","\n","# Model architecture\n","def build_model(input_features=24, timesteps=15, numResponses=22):\n","    inputs = Input(shape=(timesteps, input_features))\n","    x = DomainSpecificNormalization()(inputs)\n","    x = layers.Conv1D(32, kernel_size=3, strides=1, padding='same', use_bias=False)(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation('relu')(x)\n","    x = layers.MaxPooling1D(pool_size=2, strides=1, padding='same')(x)\n","    x = layers.Conv1D(64, kernel_size=3, strides=1, padding='same', use_bias=False)(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation('relu')(x)\n","    x = layers.MaxPooling1D(pool_size=2, strides=1, padding='same')(x)\n","    x = SimpleGNNLayer(num_features=64)(x)\n","    x = layers.Bidirectional(layers.LSTM(500, return_sequences=True))(x)\n","    x = layers.Dropout(0.2)(x)\n","    attn_output = layers.MultiHeadAttention(num_heads=4, key_dim=64)(x, x)\n","    x = layers.Add()([x, attn_output])\n","    x = layers.LayerNormalization()(x)\n","    x = layers.Dense(400, activation='relu')(x)\n","    x = layers.Dense(200, activation='relu')(x)\n","    x = layers.GlobalAveragePooling1D()(x)\n","    outputs = layers.Dense(numResponses, activation='linear')(x)\n","    return Model(inputs, outputs)\n","\n","# Data utilities\n","def load_subject_data(subjects_to_load, base_path):\n","    all_data = []\n","    for i in subjects_to_load:\n","        data_path = f\"{base_path}/S{i}_E1.mat\"\n","        subject_data = loadmat(data_path)[\"Data\"]\n","        all_data.append(subject_data)\n","    return all_data\n","\n","def split_data(data, sequence_length=15, n_features=24):\n","    X = data[:, 36:58]\n","    Z = data[:, 58:82]\n","    n_sequences = len(Z) - sequence_length + 1\n","    inputs = np.zeros((n_sequences, sequence_length, n_features))\n","    outputs = np.zeros((n_sequences, X.shape[1]))\n","    for j in range(n_sequences):\n","        inputs[j] = Z[j:j + sequence_length]\n","        outputs[j] = X[j + sequence_length - 1]\n","    return inputs, outputs\n","\n","# Fitness function\n","def fitness_function(subject_selection, all_subjects_data, test_subject_data):\n","    try:\n","        # Ensure at least 5 subjects are selected\n","        selected_indices = [i for i, val in enumerate(subject_selection) if val == 1]\n","        if len(selected_indices) < 5:\n","            return -1e6  # Penalize invalid selection\n","\n","        # Combine data from selected subjects\n","        selected_data = [all_subjects_data[i] for i in selected_indices]\n","        combined_data = np.concatenate(selected_data, axis=0)\n","        in_train, out_train = split_data(combined_data)\n","        in_val, out_val = split_data(test_subject_data)\n","\n","        # Train the model\n","        model = build_model()\n","        model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n","        model.fit(in_train, out_train, validation_data=(in_val, out_val), epochs=10, verbose=1)\n","\n","        # Predict on validation data\n","        predictions = model.predict(in_val)\n","\n","        # Compute Pearson correlation coefficient for each DOF\n","        correlations = []\n","        for i in range(predictions.shape[1]):\n","            corr, _ = pearsonr(out_val[:, i], predictions[:, i])\n","            correlations.append(corr)\n","\n","        # Return negative mean CC (maximize CC by minimizing negative CC)\n","        return -np.mean(correlations)\n","\n","    except Exception as e:\n","        print(f\"Error in fitness function: {str(e)}\")\n","        return -1e6  # Penalize invalid configurations\n","\n","# Bayesian Optimization\n","def bayesian_optimization(all_subjects_data, test_subject_data, num_trials=50):\n","    def objective(trial):\n","        # Generate binary vector for subject selection\n","        subject_selection = [\n","            trial.suggest_int(f\"subject_{i}\", 0, 1) for i in range(len(all_subjects_data))\n","        ]\n","        # Evaluate fitness function\n","        return fitness_function(subject_selection, all_subjects_data, test_subject_data)\n","\n","    # Create and run Bayesian optimization study\n","    study = optuna.create_study(direction=\"minimize\")  # Minimize negative CC\n","    study.optimize(objective, n_trials=num_trials)\n","\n","    # Extract the best subset\n","    best_params = study.best_params\n","    best_selection = [best_params[f\"subject_{i}\"] for i in range(len(all_subjects_data))]\n","    return best_selection, -study.best_value  # Return positive CC\n","\n","def train_and_test_model(best_selection, all_subjects_data, test_subject_data):\n","    selected_indices = [i for i, selected in enumerate(best_selection) if selected == 1]\n","    selected_data = [all_subjects_data[i] for i in selected_indices]\n","    combined_data = np.concatenate(selected_data, axis=0)\n","    print(combined_data.shape)\n","    in_train, out_train = split_data(combined_data)\n","    print(in_train.shape)\n","    print(out_train.shape)\n","    in_test, out_test = split_data(test_subject_data)\n","    print(in_test.shape)\n","    print(out_test.shape)\n","\n","    # Build and train the model\n","    model = build_model()\n","    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n","    model.fit(in_train, out_train, validation_split=0.2, epochs=150, batch_size=32, verbose=1)\n","\n","    # Make predictions\n","    predictions = model.predict(in_test)\n","\n","    # Compute Pearson correlation coefficients for each DOF\n","    correlations = [pearsonr(out_test[:, i], predictions[:, i])[0] for i in range(predictions.shape[1])]\n","\n","    # Plot predicted vs actual for each DOF\n","    for i in range(predictions.shape[1]):  # Loop through each degree of freedom\n","        n_rows = out_test.shape[0]\n","        x = np.arange(n_rows)\n","\n","        y1 = out_test[:, i]  # Actual values for this DOF\n","        y2 = predictions[:, i]  # Predicted values for this DOF\n","\n","        plt.figure(figsize=(12, 6))\n","        plt.plot(x, y1, label='Actual', color='blue')  # Actual line\n","        plt.plot(x, y2, label='Predicted', color='red')  # Predicted line\n","\n","        # Title with mean and current correlation coefficient\n","        cc_current = correlations[i]  # Current DOF correlation\n","        plot_title = f\"DOF {i+1} - CC Current: {cc_current:.2f}, CC Mean: {np.mean(correlations):.2f}\"\n","\n","        plt.title(plot_title)\n","        plt.xlabel('Index')\n","        plt.ylabel('Values')\n","        plt.legend()\n","        plt.show()\n","\n","    # Return correlations and their mean\n","    return correlations, np.mean(correlations)\n","\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1UkFsUks_xVHMoZmouaWgKXsdNiMmP8uB"},"id":"Hv8eJxcHiv40","executionInfo":{"status":"ok","timestamp":1735303934136,"user_tz":-240,"elapsed":24716475,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"}},"outputId":"98590002-fc7b-4318-9e71-77d2093d4ebf"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Main program\n","if __name__ == \"__main__\":\n","    base_path = '/content/drive/My Drive/Colab Notebooks/processed withstim'\n","    all_subjects = list(range(1, 41))  # Use 40 subjects\n","    test_subject = 1\n","    train_subjects = [s for s in all_subjects if s != test_subject]\n","\n","    # Load data\n","    all_subjects_data = load_subject_data(train_subjects, base_path)\n","    test_subject_data = load_subject_data([test_subject], base_path)[0]\n","\n","    # Bayesian Optimization\n","    print(\"Starting Bayesian Optimization...\")\n","    bo_selection, best_cc = bayesian_optimization(all_subjects_data, test_subject_data, num_trials=70)\n","    print(f\"Best selection: {bo_selection}\")\n","    print(f\"Best average CC: {best_cc}\")\n","\n","    # Train and test the model\n","    correlations, avg_correlation = train_and_test_model(bo_selection, all_subjects_data, test_subject_data)\n","    print(f\"Correlations per degree of freedom: {correlations}\")\n","    print(f\"Average Pearson correlation coefficient: {avg_correlation}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyOmMV58DY8A9ZMG5RlTAgUa"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}