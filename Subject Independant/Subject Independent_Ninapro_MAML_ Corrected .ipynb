{"cells":[{"cell_type":"markdown","metadata":{"id":"FVw_sHiQav9Y"},"source":["### **Connecting with Drive**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18625,"status":"ok","timestamp":1738821655627,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"},"user_tz":-240},"id":"3fuk29ccavxQ","outputId":"2c0da54b-1404-4b17-99d1-0b8a27ebed13"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#Reading the training data Subject\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"OWU0BwfKaqMa"},"source":["### **Importing All needed libraries**"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5014,"status":"ok","timestamp":1738821660640,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"},"user_tz":-240},"id":"zvMx9ro8ZntG"},"outputs":[],"source":["#Importing all needed libraries\n","import pandas as pd\n","import numpy as np #Matric math\n","import tensorflow as tf #ML\n","from tensorflow.python.framework import ops\n","from random import randint\n","from numpy import array\n","from numpy import argmax\n","import keras.backend as K\n","from tensorflow.keras import models\n","from numpy import array_equal\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import LSTM, Bidirectional\n","from tensorflow.keras.layers import Dense, Flatten\n","from tensorflow.keras import Input\n","from tensorflow.keras.layers import TimeDistributed\n","from tensorflow.keras.layers import RepeatVector\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.utils import plot_model\n","import matplotlib.pyplot as plt\n","import sys\n","import os\n","from scipy.io import loadmat\n","from scipy.io import loadmat\n","ops.reset_default_graph()\n","\n","tf.compat.v1.reset_default_graph() #Clearning cache\n","sess=tf.compat.v1.InteractiveSession()\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f27nYeB4a4F9"},"source":["### **Define Task**"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"rSSsBybQa36e","executionInfo":{"status":"ok","timestamp":1738821660641,"user_tz":-240,"elapsed":4,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"}}},"outputs":[],"source":["\n","\n","def split_task_data(data, sequence_length=15, n_features=96, split_ratio=0.8):\n","    Z = np.array(data[:, :n_features], dtype=np.float32)  # Ensure correct dtype\n","    X = np.array(data[:, n_features:n_features+22], dtype=np.float32)  # Ensure correct slicing\n","\n","    n_sequences = len(Z) - sequence_length + 1\n","    inputs = np.zeros((n_sequences, sequence_length, n_features), dtype=np.float32)\n","    outputs = np.zeros((n_sequences, X.shape[1]), dtype=np.float32)\n","\n","    for j in range(n_sequences):\n","        inputs[j] = Z[j:j + sequence_length]\n","        outputs[j] = X[j + sequence_length - 1]\n","\n","    split_index = int(len(inputs) * split_ratio)\n","    return inputs[:split_index], outputs[:split_index], inputs[split_index:], outputs[split_index:]\n","\n","\n","# Prepare tasks for MAML\n","def prepare_tasks(subjects, base_path, sequence_length=15, n_features=96):\n","    tasks = []\n","    for subject in subjects:\n","        data_path = f\"{base_path}/S{subject}_E1.mat\"\n","        subject_data = loadmat(data_path)[\"Data\"]\n","        print(f\"Shape of subject {subject}: {subject_data.shape}\")\n","        support_x, support_y, query_x, query_y = split_task_data(subject_data, sequence_length, n_features)\n","        print(f\"Shape of support_x for subject {subject}: {support_x.shape}\")\n","        print(f\"Shape of support_y for subject {subject}: {support_y.shape}\")\n","        print(f\"Shape of query_x for subject {subject}: {query_x.shape}\")\n","        print(f\"Shape of query_y for subject {subject}: {query_y.shape}\")\n","        tasks.append((support_x, support_y, query_x, query_y))\n","\n","    return tasks\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3O4YAmlwccNi"},"source":["### **build model**"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"YmPV-5jvccB0","executionInfo":{"status":"ok","timestamp":1738821660641,"user_tz":-240,"elapsed":4,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"}}},"outputs":[],"source":["from tensorflow.keras import layers, Model, Input\n","\n","def build_model(input_features=96, timesteps=15, output_dim=22):\n","    inputs = Input(shape=(timesteps, input_features))\n","\n","    # CNN Layers\n","    x = layers.Conv1D(32, kernel_size=3, strides=1, padding='same', activation='relu')(inputs)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.MaxPooling1D(pool_size=2, strides=1, padding='same')(x)\n","\n","    x = layers.Conv1D(64, kernel_size=3, strides=1, padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.MaxPooling1D(pool_size=2, strides=1, padding='same')(x)\n","\n","    x = layers.Conv1D(128, kernel_size=3, strides=1, padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.MaxPooling1D(pool_size=2, strides=1, padding='same')(x)\n","\n","    # LSTM Layers\n","    x = layers.Bidirectional(layers.LSTM(500, return_sequences=True))(x)\n","    x = layers.Dropout(0.2)(x)\n","\n","    x = layers.Bidirectional(layers.LSTM(500))(x)\n","    x = layers.Dropout(0.2)(x)\n","\n","    # Fully Connected Layer\n","    outputs = layers.Dense(output_dim, activation='linear')(x)\n","    return Model(inputs, outputs)\n","\n","\n","def build_deep_cnn_lstm(input_features=96, timesteps=15, output_dim=22):\n","    inputs = Input(shape=(timesteps, input_features))\n","\n","    # CNN Block 1\n","    x = layers.Conv1D(64, 5, padding='same', activation='relu')(inputs)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.MaxPooling1D(2)(x)\n","    skip1 = x  # Save for later residual connection\n","\n","    # CNN Block 2\n","    x = layers.Conv1D(128, 3, padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.MaxPooling1D(2)(x)\n","    skip2 = x  # Save for later residual connection\n","\n","    # CNN Block 3 with residual\n","    x = layers.Conv1D(256, 3, padding='same', activation='relu')(x)\n","    x = layers.BatchNormalization()(x)\n","\n","    # ðŸ”¥ Fix: Ensure `skip2` has the same shape as `x` before addition\n","    skip2 = layers.Conv1D(256, 1, padding='same', activation='linear')(skip2)  # ðŸ”¹ Match dimensions\n","    x = layers.add([x, skip2])  # Now both have (batch_size, timesteps, 256)\n","\n","    # LSTM Block\n","    x = layers.Bidirectional(layers.LSTM(512, return_sequences=True))(x)\n","    x = layers.Bidirectional(layers.LSTM(256))(x)\n","\n","    # Dense Block\n","    x = layers.Dense(128, activation='relu')(x)\n","    outputs = layers.Dense(output_dim, activation='linear')(x)\n","\n","    return Model(inputs, outputs)\n","\n","\n","def build_lstm_attention(input_features=96, timesteps=15, output_dim=22):\n","    inputs = Input(shape=(timesteps, input_features))\n","\n","    # Bi-LSTM Layer\n","    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(inputs)\n","\n","    # Attention Mechanism\n","    query = layers.Dense(256)(x)\n","    value = layers.Dense(256)(x)\n","    attention = layers.Attention()([query, value])\n","\n","    # Temporal Pooling\n","    x = layers.GlobalAveragePooling1D()(attention)\n","    x = layers.Dropout(0.3)(x)\n","\n","    outputs = layers.Dense(output_dim, activation='linear')(x)\n","    return Model(inputs, outputs)\n","\n","def build_gru_model(input_features=96, timesteps=15, output_dim=22):\n","    inputs = Input(shape=(timesteps, input_features), dtype=tf.float32)\n","\n","    x = layers.Bidirectional(layers.GRU(256, return_sequences=True, dtype=tf.float32))(inputs)\n","    x = layers.Bidirectional(layers.GRU(128, dtype=tf.float32))(x)\n","\n","    gate = layers.Dense(256, activation='sigmoid', dtype=tf.float32)(x)\n","    x = layers.Lambda(lambda tensors: tf.multiply(tf.cast(tensors[0], tf.float32),\n","                                                   tf.cast(tensors[1], tf.float32)))([x, gate])\n","\n","    outputs = layers.Dense(output_dim, activation='linear', dtype=tf.float32)(x)\n","    return Model(inputs, outputs)\n","\n","\n","def build_time_distributed_model(input_features=96, timesteps=15, output_dim=22):\n","    inputs = Input(shape=(timesteps, input_features))\n","\n","    # Time Distributed Processing\n","    x = layers.TimeDistributed(layers.Dense(128, activation='relu'))(inputs)\n","    x = layers.TimeDistributed(layers.Dense(64, activation='relu'))(x)\n","\n","    # Temporal Processing\n","    x = layers.LSTM(128, return_sequences=True)(x)\n","    x = layers.GlobalMaxPooling1D()(x)\n","\n","    outputs = layers.Dense(output_dim, activation='linear')(x)\n","    return Model(inputs, outputs)\n","\n","def build_cnn_attention_hybrid(input_features=96, timesteps=15, output_dim=22):\n","    inputs = Input(shape=(timesteps, input_features), dtype=tf.float32)  # Ensure input is float32\n","\n","    # CNN Feature Extraction\n","    x = layers.Conv1D(64, 3, padding='same', activation='relu')(inputs)\n","    x = layers.MaxPooling1D(2)(x)\n","    x = layers.Conv1D(128, 3, padding='same', activation='relu')(x)\n","\n","    # Fix: Use Lambda layer instead of `tf.cast`\n","    x = layers.Lambda(lambda t: tf.cast(t, tf.float32))(x)  # Correct way\n","\n","    # Multi-head Attention\n","    x = layers.MultiHeadAttention(num_heads=4, key_dim=64)(x, x)\n","\n","    # Pooling and FC\n","    x = layers.GlobalAveragePooling1D()(x)\n","    x = layers.Dense(256, activation='relu')(x)\n","\n","    outputs = layers.Dense(output_dim, activation='linear')(x)\n","    return Model(inputs, outputs)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Xko9re4oc81e"},"source":["### **Training Model framework**"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"zk-Pkrfmc8rn","executionInfo":{"status":"ok","timestamp":1738821660641,"user_tz":-240,"elapsed":4,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"}}},"outputs":[],"source":["def inner_loop(model, support_x, support_y, loss_fn, inner_lr):\n","    with tf.GradientTape() as tape:\n","        predictions = model(tf.cast(support_x, tf.float32), training=True)\n","        loss = loss_fn(tf.cast(support_y, tf.float32), predictions)\n","\n","    grads = tape.gradient(loss, model.trainable_variables)\n","\n","    # Ensure gradients and variables are both float32 before update\n","    grads = [tf.cast(g, tf.float32) for g in grads]\n","\n","    for var, g in zip(model.trainable_variables, grads):\n","        var.assign_sub(inner_lr * g)\n","\n","    return model.trainable_variables, loss\n","\n","\n","def maml_step(model, tasks, loss_fn, meta_optimizer, inner_lr):\n","    meta_grads = [tf.zeros_like(var, dtype=tf.float32) for var in model.trainable_variables]\n","\n","    for support_x, support_y, query_x, query_y in tasks:\n","        updated_weights, _ = inner_loop(model, support_x, support_y, loss_fn, inner_lr)\n","\n","        with tf.GradientTape() as tape:\n","            query_predictions = model(tf.cast(query_x, tf.float32), training=True)\n","            query_loss = loss_fn(tf.cast(query_y, tf.float32), query_predictions)\n","\n","        task_grads = tape.gradient(query_loss, model.trainable_variables)\n","\n","        # âœ… Ensure gradients are not None before updating\n","        task_grads = [g if g is not None else tf.zeros_like(var, dtype=tf.float32)\n","                      for g, var in zip(task_grads, model.trainable_variables)]\n","\n","        meta_grads = [meta_grad + task_grad for meta_grad, task_grad in zip(meta_grads, task_grads)]\n","\n","    meta_grads = [grad / len(tasks) for grad in meta_grads]  # Normalize gradients\n","    meta_optimizer.apply_gradients(zip(meta_grads, model.trainable_variables))\n"]},{"cell_type":"markdown","metadata":{"id":"TpUgnKUbfcN3"},"source":["### **Train Model**"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"pt3qXJx9fb-G","executionInfo":{"status":"ok","timestamp":1738821660641,"user_tz":-240,"elapsed":4,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"}}},"outputs":[],"source":["import tensorflow as tf\n","\n","def pearson_mse_loss(y_true, y_pred, alpha=0.1):\n","    \"\"\"\n","    Computes a hybrid loss function that:\n","    - Maximizes Pearson Correlation Coefficient (CC)\n","    - Minimizes Mean Squared Error (MSE)\n","\n","    Parameters:\n","    - y_true: Actual values\n","    - y_pred: Predicted values\n","    - alpha: Weighting factor for Pearson CC loss (adjustable)\n","\n","    Returns:\n","    - Combined hybrid loss (weighted sum of CC loss and MSE loss)\n","    \"\"\"\n","    # Compute Pearson CC component\n","    y_true_mean = tf.reduce_mean(y_true, axis=0)\n","    y_pred_mean = tf.reduce_mean(y_pred, axis=0)\n","\n","    numerator = tf.reduce_sum((y_true - y_true_mean) * (y_pred - y_pred_mean), axis=0)\n","    denominator = tf.sqrt(tf.reduce_sum((y_true - y_true_mean) ** 2, axis=0)) * \\\n","                  tf.sqrt(tf.reduce_sum((y_pred - y_pred_mean) ** 2, axis=0))\n","\n","    correlation = numerator / (denominator + 1e-8)  # Avoid division by zero\n","    cc_loss = -tf.reduce_mean(correlation)  # Negative since we want to maximize CC\n","\n","    # Compute MSE component\n","    mse_loss_fn = tf.keras.losses.MeanSquaredError()\n","    mse_loss = mse_loss_fn(y_true, y_pred)\n","\n","    # Combine losses with weighting factor alpha\n","    return mse_loss + alpha * cc_loss\n","\n","\n","\n","# MAML Training Loop\n","def maml_train(model, tasks, epochs, inner_lr, meta_lr):\n","    loss_fn = lambda y_true, y_pred: pearson_mse_loss(y_true, y_pred, alpha=0.1)\n","    meta_optimizer = tf.keras.optimizers.Adam(meta_lr)\n","\n","    for epoch in range(epochs):\n","        maml_step(model, tasks, loss_fn, meta_optimizer, inner_lr)\n","        print(f\"Epoch {epoch + 1}/{epochs} completed\")\n","\n","###############   MAML +Ensemble learning ###########\n","\n","def maml_train_ensemble(num_models, tasks, epochs, inner_lr, meta_lr):\n","    \"\"\"\n","    Train multiple MAML models (ensemble members) on the given tasks.\n","\n","    Parameters:\n","    - num_models: Number of models in the ensemble.\n","    - tasks: List of tasks for meta-training.\n","    - epochs: Number of meta-training epochs.\n","    - inner_lr: Inner-loop learning rate.\n","    - meta_lr: Meta-learning rate for the outer loop.\n","\n","    Returns:\n","    - ensemble: List of trained models.\n","    \"\"\"\n","    ensemble = []\n","\n","    for i in range(num_models):\n","        print(f\"Training model {i + 1}/{num_models}...\")\n","        model = build_model()  # Use the CNN-LSTM model\n","        maml_train(model, tasks, epochs, inner_lr, meta_lr)  # Meta-train the model\n","        ensemble.append(model)\n","\n","    return ensemble\n","\n","def maml_train_ensemble_diverse(ensemble, tasks, epochs, inner_lr, meta_lr):\n","    for model in ensemble:\n","        print(f\"\\nTraining {model.name}...\")\n","        maml_train(model, tasks, epochs, inner_lr, meta_lr)\n","    return ensemble\n","\n","def create_diverse_ensemble(num_models=5):\n","    architectures = [\n","        build_model,\n","        build_deep_cnn_lstm,\n","        build_lstm_attention,\n","        build_gru_model,\n","        build_time_distributed_model,\n","        build_cnn_attention_hybrid\n","    ]\n","\n","    ensemble = []\n","    for i in range(num_models):\n","        # Cycle through different architectures\n","        model_builder = architectures[i % len(architectures)]\n","        model = model_builder()\n","        print(f\"Initialized model {i+1} with {model_builder.__name__}\")\n","        ensemble.append(model)\n","\n","    return ensemble\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LHN3FbMa-BJS"},"source":["### **Testing**"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"TEZ9xPgr-A9L","executionInfo":{"status":"ok","timestamp":1738821660641,"user_tz":-240,"elapsed":3,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"}}},"outputs":[],"source":["\n","def fine_tune_ensemble(ensemble, support_x, support_y, loss_fn, inner_lr, fine_tune_steps=1, batch_size=512):\n","    \"\"\"\n","    Fine-tune each model in the ensemble using support data.\n","\n","    Parameters:\n","    - ensemble: List of models.\n","    - support_x, support_y: Support set for fine-tuning.\n","    - loss_fn: Loss function (should be Pearson CC loss).\n","    - inner_lr: Learning rate for fine-tuning.\n","    - fine_tune_steps: Number of fine-tuning steps.\n","    - batch_size: Batch size for training.\n","\n","    Returns:\n","    - fine_tuned_weights: Updated model weights after fine-tuning.\n","    \"\"\"\n","    fine_tuned_weights = []\n","\n","    for i, model in enumerate(ensemble):\n","        print(f\"Fine-tuning model {i + 1}/{len(ensemble)}...\")\n","\n","        dataset = tf.data.Dataset.from_tensor_slices((support_x, support_y)).batch(batch_size)\n","\n","        for _ in range(fine_tune_steps):\n","            for batch_x, batch_y in dataset:\n","                updated_weights, _ = inner_loop(model, batch_x, batch_y, loss_fn, inner_lr)\n","\n","        # Apply fine-tuned weights back to the model\n","        for var, w in zip(model.trainable_variables, updated_weights):\n","            var.assign(w)\n","\n","        fine_tuned_weights.append(updated_weights)\n","\n","    return fine_tuned_weights\n","\n","def ensemble_predict_weighted_performance(ensemble, fine_tuned_weights, query_x, val_x, val_y):\n","    \"\"\"\n","    Compute ensemble predictions using weighted averaging based on Pearson CC,\n","    but using validation data instead of test data.\n","\n","    Parameters:\n","    - ensemble: List of trained models.\n","    - fine_tuned_weights: Fine-tuned weights for each model.\n","    - query_x: Input features for the test set.\n","    - val_x: Validation input features (used to compute weights).\n","    - val_y: Validation ground truth (used to compute weights).\n","\n","    Returns:\n","    - weighted_predictions: Final ensemble predictions for the test set.\n","    \"\"\"\n","    all_predictions = []\n","    cc_values = []\n","\n","    for i, (model, weights) in enumerate(zip(ensemble, fine_tuned_weights)):\n","        print(f\"Computing weight for model {i + 1}/{len(ensemble)}...\")\n","\n","        # Apply fine-tuned weights before predicting\n","        for var, w in zip(model.trainable_variables, weights):\n","            var.assign(w)\n","\n","        # Predict on validation data, NOT test data\n","        val_predictions = model(val_x, training=False)\n","        all_predictions.append(model(query_x, training=False))  # Still predict on test set\n","\n","        # Compute Pearson CC based on validation data\n","        cc = np.array([\n","            pearsonr(val_y[:, i], val_predictions[:, i])[0] for i in range(val_y.shape[1])\n","        ])\n","        cc_values.append(cc)\n","\n","    all_predictions = np.array(all_predictions)  # Shape: (num_models, batch_size, output_dim)\n","    cc_values = np.array(cc_values)  # Shape: (num_models, output_dim)\n","\n","    # Normalize correlation values as weights\n","    cc_values = np.maximum(cc_values, 0)  # Remove negative correlations (set to zero)\n","    weights = cc_values / (np.sum(cc_values, axis=0, keepdims=True) + 1e-8)  # Avoid division by zero\n","\n","    # Weighted average using CC-based weights\n","    weighted_predictions = np.sum(all_predictions * weights[:, None, :], axis=0)\n","\n","    print(f\"Model Weights (based on validation correlation coefficient): {weights}\")\n","    return weighted_predictions\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"q81rczMMDwJR"},"source":["### **Plot the result**"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"23D8Vr_-Dvs0","executionInfo":{"status":"ok","timestamp":1738821660641,"user_tz":-240,"elapsed":3,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.stats import pearsonr\n","\n","\n","def normalize_predictions(predictions):\n","    mean = np.mean(predictions, axis=0)\n","    std = np.std(predictions, axis=0) + 1e-8  # Avoid division by zero\n","    return (predictions - mean) / std\n","def scale_predictions(predictions):\n","    min_val = np.min(predictions, axis=0)\n","    max_val = np.max(predictions, axis=0)\n","    return (predictions - min_val) / (max_val - min_val)\n","def smooth_predictions(predictions, window_size=5):\n","    smoothed = np.zeros_like(predictions)\n","    for i in range(predictions.shape[1]):  # Loop through each DOF\n","        smoothed[:, i] = np.convolve(predictions[:, i], np.ones(window_size)/window_size, mode='same')\n","        smoothed[:, i] = np.convolve(predictions[:, i], np.ones(window_size)/window_size, mode='same')\n","    return smoothed\n","\n","def evaluate_and_plot(model, updated_weights, query_x, query_y, apply_smoothing=True, apply_normalization=True,apply_Scale=True):\n","    \"\"\"\n","    Evaluate the model for each DOF, calculate CC, and plot actual vs. normalized or smoothed predictions.\n","\n","    Parameters:\n","    - model: The trained model.\n","    - updated_weights: Fine-tuned weights for the task.\n","    - query_x: Input features for the query set.\n","    - query_y: Ground truth outputs for the query set.\n","    - apply_smoothing: Whether to smooth predictions.\n","    - apply_normalization: Whether to normalize predictions.\n","\n","    Returns:\n","    - correlations: List of CC values for each DOF.\n","    - avg_correlation: Average CC across all DOFs.\n","    \"\"\"\n","    # Predict using the updated weights\n","    query_predictions = model(query_x, training=False)\n","\n","    # Normalize or smooth predictions\n","\n","    if apply_smoothing:\n","        query_predictions = smooth_predictions(query_predictions)\n","    if apply_normalization:\n","        query_predictions = normalize_predictions(query_predictions)\n","    if apply_Scale:\n","        query_predictions = scale_predictions(query_predictions)\n","\n","    # Initialize lists for storing CC values\n","    correlations = []\n","\n","    # Loop through each DOF (dimension)\n","    for i in range(query_y.shape[1]):\n","        # Calculate Pearson Correlation Coefficient\n","        cc_value, _ = pearsonr(query_y[:, i], query_predictions[:, i])\n","        correlations.append(cc_value)\n","\n","        # Plot Actual vs Predicted for the current DOF\n","        plt.figure(figsize=(10, 6))\n","        plt.plot(query_y[:, i], label='Actual', color='blue')\n","        plt.plot(query_predictions[:, i], label='Predicted', color='red')\n","\n","        # Add title and labels\n","        plt.title(f'Degree of Freedom {i + 1}: CC = {cc_value:.2f}')\n","        plt.xlabel('Time Steps')\n","        plt.ylabel('Values')\n","        plt.legend()\n","        plt.show()\n","\n","    # Compute the average CC across all DOFs\n","    avg_correlation = np.mean(correlations)\n","    print(f\"Average Pearson Correlation Coefficient: {avg_correlation:.2f}\")\n","\n","    return correlations, avg_correlation\n","\n","def evaluate_ensemble(ensemble_predictions, query_y, apply_smoothing=True, apply_normalization=True, apply_scale=True):\n","    \"\"\"\n","    Evaluate ensemble predictions and plot results.\n","\n","    Parameters:\n","    - ensemble_predictions: Predictions from the ensemble.\n","    - query_y: Ground truth for the query set.\n","    - apply_smoothing: Whether to smooth predictions.\n","    - apply_normalization: Whether to normalize predictions.\n","    - apply_scale: Whether to scale predictions.\n","\n","    Returns:\n","    - correlations: List of CC values for each DOF.\n","    - avg_correlation: Average CC across all DOFs.\n","    \"\"\"\n","    # Process predictions\n","    if apply_smoothing:\n","        ensemble_predictions = smooth_predictions(ensemble_predictions)\n","    if apply_normalization:\n","        ensemble_predictions = normalize_predictions(ensemble_predictions)\n","    if apply_scale:\n","        ensemble_predictions = scale_predictions(ensemble_predictions)\n","\n","    # Initialize lists for storing CC values\n","    correlations = []\n","\n","    # Loop through each DOF (dimension)\n","    for i in range(query_y.shape[1]):\n","        # Calculate Pearson Correlation Coefficient\n","        cc_value, _ = pearsonr(query_y[:, i], ensemble_predictions[:, i])\n","        correlations.append(cc_value)\n","\n","        # Plot Actual vs Predicted for the current DOF\n","        plt.figure(figsize=(10, 6))\n","        plt.plot(query_y[:, i], label='Actual', color='blue')\n","        plt.plot(ensemble_predictions[:, i], label='Predicted', color='red')\n","\n","        # Add title and labels\n","        plt.title(f'Degree of Freedom {i + 1}: CC = {cc_value:.2f}')\n","        plt.xlabel('Time Steps')\n","        plt.ylabel('Values')\n","        plt.legend()\n","        plt.show()\n","\n","    # Compute the average CC across all DOFs\n","    avg_correlation = np.mean(correlations)\n","    print(f\"Average Pearson Correlation Coefficient: {avg_correlation:.2f}\")\n","\n","    return correlations, avg_correlation\n","\n","\n","def diverse_ensemble_predict(ensemble, query_x):\n","    predictions = []\n","    for model in ensemble:\n","        # Use different preprocessing for different models if needed\n","        pred = model.predict(query_x)\n","        predictions.append(pred)\n","\n","    # Weighted averaging (could implement more sophisticated fusion)\n","    return np.mean(predictions, axis=0)\n"]},{"cell_type":"markdown","metadata":{"id":"xvSyr83aRWRk"},"source":["## **Methods for saving the data**"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"aVoyRDGSavDh","executionInfo":{"status":"ok","timestamp":1738821660641,"user_tz":-240,"elapsed":3,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"}}},"outputs":[],"source":["# Save data in Excel file\n","def savedata(i, correlations):\n","    \"\"\"\n","    Save the correlations data in an Excel file at the specified row.\n","\n","    Parameters:\n","    - i: Integer representing the subject number (e.g., 5 for row name 'S5').\n","    - correlations: List of correlation values to save in the row.\n","    \"\"\"\n","    import openpyxl\n","    from google.colab import drive\n","\n","    # Mount Google Drive\n","    drive.mount('/content/drive')\n","\n","    # Path to the workbook\n","    workbook_path = '/content/drive/My Drive/Colab Notebooks/dataset/Ninapro MAML-8features- 6models_Subjectindependant.xlsx'\n","\n","    # Load the existing workbook\n","    wb = openpyxl.load_workbook(workbook_path)\n","\n","    # Select the active sheet\n","    sheet = wb.active\n","\n","    # Dynamically set the row name based on 'i'\n","    row_name = f'S{i}'  # Example: 'S5' if i=5\n","\n","    # Find the row with the specified row name\n","    target_row = None\n","    for row in sheet.iter_rows(min_row=1, max_row=sheet.max_row):\n","        if row[0].value == row_name:\n","            target_row = row[0].row\n","            break\n","\n","    if target_row is None:\n","        print(f\"Error: Row with name '{row_name}' not found in the sheet.\")\n","        return\n","\n","    # Write the data to the found row, starting from the second column\n","    for col, value in enumerate(correlations, start=2):  # Start at column 2 to skip the first column for the label\n","        sheet.cell(row=target_row, column=col, value=value)\n","\n","    # Save the workbook\n","    wb.save(workbook_path)\n","    print(f\"Data successfully saved for row '{row_name}'.\")\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"DRS9fwzrEBz8","executionInfo":{"status":"ok","timestamp":1738821660641,"user_tz":-240,"elapsed":3,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"}}},"outputs":[],"source":["import random\n","def process_subject(test_subject, ensemble, base_path, train_subjects, fine_tune_subjects=5, val_ratio=0.2):\n","    print(f\"Processing Test Subject: S{test_subject}\")\n","\n","    test_data = loadmat(f\"{base_path}/S{test_subject}_E1.mat\")[\"Data\"]\n","\n","    Z = test_data[:, :96]\n","    X = test_data[:, 96:118]\n","\n","    sequence_length = 15\n","    n_sequences = len(Z) - sequence_length + 1\n","\n","    query_x = np.zeros((n_sequences, sequence_length, 96), dtype=np.float32)\n","    query_y = np.zeros((n_sequences, X.shape[1]), dtype=np.float32)\n","\n","    for j in range(n_sequences):\n","        query_x[j] = Z[j:j + sequence_length]\n","        query_y[j] = X[j + sequence_length - 1]\n","\n","    # Ensure test subject is NEVER in fine-tuning set\n","    fine_tune_subjects_list = random.sample([s for s in train_subjects if s != test_subject], fine_tune_subjects)\n","    print(f\"Fine-tuning with {fine_tune_subjects_list}\")\n","\n","    support_x_list, support_y_list = [], []\n","    val_x_list, val_y_list = [], []\n","\n","    for subject in fine_tune_subjects_list:\n","        train_data = loadmat(f\"{base_path}/S{subject}_E1.mat\")[\"Data\"]\n","        support_x, support_y, val_x, val_y = split_task_data(train_data, split_ratio=1 - val_ratio)\n","        support_x_list.append(support_x)\n","        support_y_list.append(support_y)\n","        val_x_list.append(val_x)\n","        val_y_list.append(val_y)\n","\n","    support_x = np.concatenate(support_x_list, axis=0)\n","    support_y = np.concatenate(support_y_list, axis=0)\n","    val_x = np.concatenate(val_x_list, axis=0)\n","    val_y = np.concatenate(val_y_list, axis=0)\n","\n","    # Ensure fine-tuning uses the correct loss\n","    fine_tuned_weights = fine_tune_ensemble(ensemble, support_x, support_y, pearson_mse_loss, 0.01)\n","\n","    # Compute ensemble weights using validation data, NOT test data\n","    ensemble_predictions = ensemble_predict_weighted_performance(ensemble, fine_tuned_weights, query_x, val_x, val_y)\n","\n","    return evaluate_ensemble(ensemble_predictions, query_y, apply_smoothing=True, apply_normalization=False, apply_scale=True)\n"]},{"cell_type":"markdown","metadata":{"id":"_FOAcRu8eydX"},"source":["### **Main method**"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1AHjr3UjkPj54LvkSdzu3YfPyo_Zqnqcz"},"id":"RLnfXIFIeyIz","outputId":"e96025ff-b7dc-4dc8-d44c-651b82f7a297","executionInfo":{"status":"error","timestamp":1738872206581,"user_tz":-240,"elapsed":6044662,"user":{"displayName":"hend elmohandes","userId":"08667212620651588699"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["\n","import os\n","\n","if __name__ == \"__main__\":\n","    base_path = '/content/drive/My Drive/Colab Notebooks/dataset wstimi/8 Features'\n","    all_subjects = list(range(1, 41))  # Subjects 1 to 40\n","\n","    # Start resuming from test subject 10\n","    start_subject = 18\n","\n","    for test_subject in all_subjects:\n","        # Skip subjects that were already processed\n","        if test_subject < start_subject:\n","            continue  # Skip to the starting subject\n","\n","        print(f\"Processing S{test_subject}\")\n","\n","        # 1. Load data with explicit dtype\n","        train_subjects = [s for s in all_subjects if s != test_subject]\n","        tasks = prepare_tasks(train_subjects, base_path)\n","\n","        # 2. Create ensemble with dtype-consistent models\n","        ensemble = create_diverse_ensemble(num_models=6)\n","\n","        # 3. Train with dtype-safe settings\n","        trained_ensemble = maml_train_ensemble_diverse(\n","            ensemble, tasks, epochs=100, inner_lr=0.01, meta_lr=0.001\n","        )\n","\n","        # 4. Process subject with dtype-controlled data\n","        correlations, _ = process_subject(test_subject, ensemble, base_path, train_subjects)\n","\n","        # Save the results\n","        savedata(test_subject, correlations)\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyM7JFRyfCB9WDbPin7MqxoF"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}